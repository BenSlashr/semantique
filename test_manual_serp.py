#!/usr/bin/env python3
"""
Test manuel avec URLs r√©elles du TOP 5 des requ√™tes demand√©es
"""

import asyncio
from services.valueserp_service import ValueSerpService
from collections import Counter
import time

class ManualSerpTester:
    def __init__(self):
        self.service = ValueSerpService()
    
    async def test_query_manual(self, query: str, top_urls: list):
        """Test avec URLs manuelles du top 5"""
        
        print(f"\n{'='*80}")
        print(f"üîç TEST REQU√äTE MANUELLE: '{query}'")
        print(f"üìç URLs simul√©es du TOP 5 Google")
        print(f"{'='*80}")
        
        results = {
            'query': query,
            'extractions': [],
            'failed': [],
            'stats': {}
        }
        
        print(f"\nüìÑ EXTRACTION DES CONTENUS TOP 5:")
        
        for i, url in enumerate(top_urls[:5], 1):
            print(f"\n   üîç [{i}/5] Analyse de: {url}")
            
            try:
                start_time = time.time()
                content_data = await self.service._fetch_page_content(url)
                extraction_time = time.time() - start_time
                
                if content_data['word_count'] > 0:
                    extraction = {
                        'position': i,
                        'url': url,
                        'domain': self._extract_domain(url),
                        'word_count': content_data['word_count'],
                        'quality': content_data['content_quality'],
                        'content': content_data['content'],
                        'h1': content_data.get('h1', ''),
                        'author': content_data.get('author', ''),
                        'date': content_data.get('date', ''),
                        'images': content_data['images'],
                        'links': content_data['internal_links'] + content_data['external_links'],
                        'extraction_time': round(extraction_time, 2)
                    }
                    
                    results['extractions'].append(extraction)
                    
                    print(f"      ‚úÖ Succ√®s: {content_data['word_count']} mots ({content_data['content_quality']})")
                    print(f"      üìÑ H1: {content_data.get('h1', 'N/A')[:60]}...")
                    print(f"      ‚è±Ô∏è Temps: {extraction_time:.2f}s")
                    
                    if content_data.get('author'):
                        print(f"      üë§ Auteur: {content_data['author']}")
                    if content_data.get('date'):
                        print(f"      üìÖ Date: {content_data['date']}")
                else:
                    results['failed'].append({
                        'url': url,
                        'reason': 'Aucun contenu extrait'
                    })
                    print(f"      ‚ùå √âchec: Aucun contenu extrait")
                    
            except Exception as e:
                results['failed'].append({
                    'url': url,
                    'reason': str(e)
                })
                print(f"      ‚ùå Erreur: {str(e)[:80]}...")
        
        # Analyse des mots-cl√©s
        if results['extractions']:
            self._analyze_extractions(results)
        
        self._display_summary(results)
        return results
    
    def _extract_domain(self, url):
        """Extrait le domaine d'une URL"""
        from urllib.parse import urlparse
        try:
            return urlparse(url).netloc.replace('www.', '')
        except:
            return url
    
    def _analyze_extractions(self, results):
        """Analyse s√©mantique des extractions"""
        
        # Concat√©nation de tous les contenus
        all_content = " ".join([ext['content'] for ext in results['extractions']])
        query = results['query']
        
        # Analyse basique des mots-cl√©s
        import re
        clean_content = re.sub(r'[^\w\s\'-]', ' ', all_content.lower())
        words = [word for word in clean_content.split() if len(word) >= 3]
        
        # Stop words fran√ßais
        stop_words = {
            'le', 'la', 'les', 'un', 'une', 'des', 'du', 'de', 'et', 'ou', '√†', 'ce', 'se',
            'que', 'qui', 'dont', 'o√π', 'il', 'elle', 'nous', 'vous', 'ils', 'elles', 'on',
            'pour', 'par', 'avec', 'sans', 'dans', 'sur', 'sous', 'vers', 'entre', 'chez',
            'plus', 'moins', 'tr√®s', 'bien', 'mal', 'tout', 'tous', 'avoir', '√™tre', 'faire',
            'dire', 'aller', 'voir', 'savoir', 'pouvoir', 'vouloir', 'venir', 'falloir',
            'depuis', 'pendant', 'apr√®s', 'avant', 'encore', 'd√©j√†', 'toujours', 'jamais'
        }
        
        meaningful_words = [word for word in words if word not in stop_words and word.isalpha()]
        word_freq = Counter(meaningful_words)
        
        # Mots li√©s √† la requ√™te
        query_words = set(query.lower().split())
        related_words = {}
        for word, freq in word_freq.most_common(50):
            for query_word in query_words:
                if query_word in word or word in query_word or len(query_word) > 4 and query_word[:4] in word:
                    related_words[word] = freq
                    break
        
        # Bi-grammes
        bigrams = []
        for i in range(len(meaningful_words) - 1):
            bigram = f"{meaningful_words[i]} {meaningful_words[i+1]}"
            bigrams.append(bigram)
        
        bigram_freq = Counter(bigrams)
        
        results['analysis'] = {
            'total_words': len(words),
            'meaningful_words': len(meaningful_words),
            'unique_words': len(set(meaningful_words)),
            'top_words': word_freq.most_common(20),
            'query_related': list(related_words.items())[:15],
            'top_bigrams': bigram_freq.most_common(10)
        }
    
    def _display_summary(self, results):
        """Affiche le r√©sum√© des r√©sultats"""
        
        print(f"\n{'='*80}")
        print(f"üìä R√âSUM√â - '{results['query']}'")
        print(f"{'='*80}")
        
        successful = len(results['extractions'])
        failed = len(results['failed'])
        total_words = sum([ext['word_count'] for ext in results['extractions']])
        
        print(f"\nüìà STATISTIQUES EXTRACTION:")
        print(f"   ‚úÖ R√©ussies: {successful}/5")
        print(f"   ‚ùå √âchou√©es: {failed}/5")
        print(f"   üìù Mots totaux: {total_words:,}")
        print(f"   üìä Moyenne: {total_words // max(successful, 1):,} mots/site")
        
        if results['extractions']:
            print(f"\nüèÜ CLASSEMENT DES SITES:")
            for ext in sorted(results['extractions'], key=lambda x: x['word_count'], reverse=True):
                print(f"   #{ext['position']} {ext['domain']:<30} {ext['word_count']:>5,} mots ({ext['quality']})")
        
        if 'analysis' in results:
            analysis = results['analysis']
            print(f"\nüß† ANALYSE S√âMANTIQUE:")
            print(f"   üìù Mots significatifs: {analysis['meaningful_words']:,}")
            print(f"   üíé Mots uniques: {analysis['unique_words']:,}")
            print(f"   üìä Richesse: {analysis['unique_words']/max(analysis['meaningful_words'],1):.2f}")
            
            print(f"\n   üîù TOP 15 MOTS-CL√âS:")
            for i, (word, freq) in enumerate(analysis['top_words'][:15], 1):
                print(f"      {i:2}. {word:<20} ({freq:>3}x)")
            
            if analysis['query_related']:
                print(f"\n   üéØ MOTS LI√âS √Ä '{results['query']}':")
                for word, freq in analysis['query_related'][:10]:
                    print(f"      ‚Ä¢ {word:<20} ({freq:>3}x)")
            
            print(f"\n   üîó TOP EXPRESSIONS:")
            for i, (bigram, freq) in enumerate(analysis['top_bigrams'][:8], 1):
                print(f"      {i}. \"{bigram}\" ({freq}x)")
        
        if results['failed']:
            print(f"\n‚ùå √âCHECS D'EXTRACTION:")
            for fail in results['failed']:
                print(f"   - {self._extract_domain(fail['url'])}: {fail['reason'][:50]}...")
    
    async def test_all_queries(self):
        """Test sur les 3 requ√™tes avec URLs manuelles"""
        
        # URLs r√©elles approximatives du TOP 5 (bas√©es sur une recherche manuelle)
        query_urls = {
            "nettoyage apr√®s inondation": [
                "https://www.service-public.fr/particuliers/vosdroits/F3050",
                "https://www.economie.gouv.fr/dgccrf/Publications/Vie-pratique/Fiches-pratiques/Degats-eaux-inondations",
                "https://www.lci.fr/societe/inondations-comment-nettoyer-sa-maison-apres-une-inondation-2131847.html",
                "https://www.maif.fr/conseils-prevention/la-maison/degats-des-eaux-inondations.html",
                "https://www.generali.fr/assurance-habitation/degats-des-eaux/nettoyage-maison-inondee/"
            ],
            "etat surface acier": [
                "https://fr.wikipedia.org/wiki/√âtat_de_surface",
                "https://www.technologuepro.com/cours-genie-mecanique/cours-cotation-tolerancement/etats-surfaces.html",
                "https://www.mitutoyo.fr/webfoo/wp-content/uploads/Etat_de_surface.pdf",
                "https://www.directindustry.fr/fabricant-industriel/etat-surface-88991.html",
                "https://www.sandvik.coromant.com/fr-fr/knowledge/general-turning/roughness-and-surface-finish"
            ],
            "aide implantation": [
                "https://www.service-public.fr/professionnels/vosdroits/F22316",
                "https://www.economie.gouv.fr/cedef/creation-entreprise-aides-financieres",
                "https://www.bpifrance.fr/nos-solutions/financer-mon-projet-de-creation-reprise",
                "https://www.pole-emploi.fr/candidat/creation-entreprise/les-aides.html",
                "https://www.urssaf.fr/accueil/aides-et-mesures-covid-19/toutes-les-mesures-daide/exonerations.html"
            ]
        }
        
        print("üöÄ TEST MANUEL SUR REQU√äTES GOOGLE R√âELLES")
        print("=" * 80)
        print("üìç Mode simulation avec URLs repr√©sentatives du TOP 5")
        
        all_results = []
        
        for i, (query, urls) in enumerate(query_urls.items(), 1):
            print(f"\n\nüéØ REQU√äTE {i}/{len(query_urls)}")
            result = await self.test_query_manual(query, urls)
            
            if result:
                all_results.append(result)
            
            if i < len(query_urls):
                print(f"\n‚è≥ Pause de 3 secondes avant requ√™te suivante...")
                await asyncio.sleep(3)
        
        # Comparaison finale
        if len(all_results) > 1:
            self._compare_all_queries(all_results)
        
        return all_results
    
    def _compare_all_queries(self, all_results):
        """Comparaison entre toutes les requ√™tes"""
        
        print(f"\n{'='*80}")
        print(f"üÜö COMPARAISON GLOBALE DES 3 REQU√äTES")
        print(f"{'='*80}")
        
        print(f"{'Requ√™te':<30} {'Succ√®s':<8} {'Mots':<8} {'Moy/site':<8}")
        print("-" * 60)
        
        for result in all_results:
            query = result['query'][:28]
            successful = len(result['extractions'])
            total_words = sum([ext['word_count'] for ext in result['extractions']])
            avg_words = total_words // max(successful, 1)
            
            print(f"{query:<30} {successful}/5{'':<3} {total_words:<8,} {avg_words:<8,}")
        
        print(f"\nüí° INSIGHTS COMPARATIFS:")
        
        # Requ√™te avec le plus de contenu
        max_words = max(all_results, key=lambda r: sum([ext['word_count'] for ext in r['extractions']]))
        max_total = sum([ext['word_count'] for ext in max_words['extractions']])
        print(f"   üìà Plus de contenu: '{max_words['query']}' ({max_total:,} mots)")
        
        # Meilleur taux de succ√®s
        best_success = max(all_results, key=lambda r: len(r['extractions']))
        success_rate = len(best_success['extractions'])
        print(f"   üéØ Meilleur succ√®s: '{best_success['query']}' ({success_rate}/5 sites)")
        
        # Analyse des domaines les plus repr√©sent√©s
        all_domains = []
        for result in all_results:
            for ext in result['extractions']:
                all_domains.append(ext['domain'])
        
        domain_freq = Counter(all_domains)
        if domain_freq:
            print(f"   üèÜ Domaines les plus analys√©s:")
            for domain, freq in domain_freq.most_common(3):
                print(f"      - {domain}: {freq} fois")

async def main():
    tester = ManualSerpTester()
    await tester.test_all_queries()

if __name__ == "__main__":
    asyncio.run(main())