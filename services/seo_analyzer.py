import re
import nltk
from collections import Counter, defaultdict
from typing import Dict, List, Any, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import asyncio
from statistics import mean

# T√©l√©charger les ressources NLTK n√©cessaires
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

class SEOAnalyzer:
    def __init__(self):
        self.french_stopwords = set(stopwords.words('french'))
        self.french_stopwords.update([
            'le', 'la', 'les', 'un', 'une', 'des', 'du', 'de', 'et', 'ou', 'est', 'sont',
            'dans', 'sur', 'avec', 'par', 'pour', 'sans', 'sous', 'vers', 'chez',
            'plus', 'tr√®s', 'bien', 'tout', 'tous', 'toute', 'toutes', 'que', 'qui',
            'quoi', 'dont', 'o√π', 'comment', 'pourquoi', 'quand'
        ])
        
        # Cache des stop words pour optimisation des validations
        self.validation_stop_words = frozenset({
            'de', 'du', 'des', 'le', 'la', 'les', 'un', 'une', 'ce', 'ces', 'se', 'sa', 'son', 'ses',
            'sur', 'sous', 'dans', 'avec', 'sans', 'pour', 'par', 'vers', 'chez', 'entre', 'depuis',
            'et', 'ou', 'ni', 'mais', 'car', 'donc', 'or', 'comme', 'que', 'qui', 'dont', 'o√π',
            'il', 'elle', 'nous', 'vous', 'ils', 'elles', 'je', 'tu', 'me', 'te', 'se', 'lui', 'leur',
            'mon', 'ton', 'ma', 'ta', 'mes', 'tes', 'nos', 'vos', 'leur', 'leurs', 'votre', 'notre',
            'est', 'sont', '√©tait', '√©taient', 'sera', 'seront', 'avoir', '√™tre', 'faire', 'dire',
            'aller', 'voir', 'savoir', 'pouvoir', 'vouloir', 'venir', 'falloir', 'devoir', 'prendre',
            'plus', 'moins', 'tr√®s', 'bien', 'mal', 'mieux', 'beaucoup', 'peu', 'assez', 'trop',
            'tout', 'tous', 'toute', 'toutes', 'autre', 'autres', 'm√™me', 'm√™mes', 'tel', 'telle',
            '√†', 'au', 'aux', 'en', 'y', 'ne', 'pas', 'non', 'oui', 'si', 'peut', 'peuvent'
        })
        
        # Cache des exceptions SEO
        self.seo_exceptions = frozenset({'seo', 'web', 'app', 'cms', 'api', 'roi', 'kpi', 'b2b', 'b2c'})
        
        # Cache des patterns invalides
        self.invalid_bigram_patterns = frozenset([
            '√† la', '√† le', '√† les', 'de la', 'de le', 'de les', 'du c√¥t√©',
            'en tant', 'au niveau', 'par rapport', 'gr√¢ce √†', 'face √†',
            'selon les', 'selon le', 'selon la', 'parmi les', 'parmi le'
        ])
        
        self.invalid_trigram_starts = frozenset(['il est', 'elle est', 'nous sommes', 'vous √™tes', 'ils sont', 'c est'])
        self.invalid_trigram_ends = frozenset(['de plus', 'en plus', 'en effet', 'par exemple', 'en fait'])
        
        # Cache regex compil√©es pour √©viter recompilation
        self.regex_punctuation = re.compile(r'[^\w\s]')
        self.regex_whitespace = re.compile(r'\s+')
        
    async def analyze_competition(self, query: str, serp_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyse compl√®te de la concurrence SEO"""
        
        # R√©initialisation des caches pour chaque nouvelle analyse
        self._text_cache = {}
        
        # Si pas de r√©sultats r√©els, utiliser les donn√©es de d√©monstration
        if not serp_results or not serp_results.get('organic_results'):
            return self._get_demo_analysis(query)
        
        # Extraction des donn√©es organiques et √©l√©ments SERP
        organic_results = serp_results.get('organic_results', [])
        paa_questions = serp_results.get('paa', [])
        related_searches = serp_results.get('related_searches', [])
        inline_videos = serp_results.get('inline_videos', [])
        
        all_content = self._extract_all_content(organic_results)
        query_words = self._clean_text(query).split()
        
        keywords_obligatoires = self._extract_required_keywords(all_content, query_words)
        keywords_complementaires = self._extract_complementary_keywords(all_content, keywords_obligatoires)
        
        # Ajout des statistiques min-max pour chaque mot-cl√©
        keywords_obligatoires = self._add_minmax_stats(keywords_obligatoires, organic_results)
        keywords_complementaires = self._add_minmax_stats(keywords_complementaires, organic_results)
        ngrams = self._extract_ngrams(all_content, query)
        
        # Extraction des groupes de mots-cl√©s
        bigrams = self._extract_bigrams(all_content, query)
        trigrams = self._extract_trigrams(all_content, query)
        questions = self._generate_questions(query, keywords_obligatoires, paa_questions)
        
        concurrence_analysee = self._analyze_competitors(organic_results, keywords_obligatoires)
        
        score_target = self._calculate_target_score(concurrence_analysee)
        mots_requis = self._calculate_required_words(concurrence_analysee)
        max_suroptimisation = self._calculate_max_overoptimization(keywords_obligatoires)
        
        type_analysis = self._analyze_content_types(organic_results)
        word_stats = self._calculate_word_statistics(organic_results)
        
        return {
            "query": query,
            "score_target": score_target,
            "mots_requis": mots_requis,
            "KW_obligatoires": keywords_obligatoires,
            "KW_complementaires": keywords_complementaires,
            "ngrams": ngrams,
            "max_suroptimisation": max_suroptimisation,
            "questions": questions,
            "paa": paa_questions,
            "related_searches": related_searches,
            "inline_videos": inline_videos,
            "bigrams": bigrams,
            "trigrams": trigrams,
            "type_editorial": type_analysis["editorial"],
            "type_catalogue": type_analysis["catalogue"],
            "type_fiche_produit": type_analysis["fiche_produit"],
            "mots_uniques_min_max_moyenne": word_stats,
            "concurrence": concurrence_analysee
        }
    
    def _extract_all_content(self, serp_results: List[Dict[str, Any]]) -> str:
        """Extrait tout le contenu textuel des r√©sultats SERP"""
        all_text = []
        
        for result in serp_results:
            text_parts = [
                result.get("title", ""),
                result.get("h1", ""),
                result.get("h2", ""),
                result.get("h3", ""),
                result.get("content", ""),
                result.get("snippet", "")
            ]
            all_text.extend([part for part in text_parts if part])
        
        return " ".join(all_text)
    
    def _clean_text(self, text: str) -> str:
        """Nettoie et normalise le texte avec cache pour optimisation"""
        if not text:
            return ""
            
        # Cache basique pour √©viter recalculer les m√™mes textes
        text_hash = hash(text)
        if hasattr(self, '_text_cache') and text_hash in self._text_cache:
            return self._text_cache[text_hash]
        
        # Utilisation des regex pr√©compil√©es
        cleaned = self.regex_punctuation.sub(' ', text.lower())
        cleaned = self.regex_whitespace.sub(' ', cleaned).strip()
        
        # Cache le r√©sultat si le cache n'est pas trop gros
        if not hasattr(self, '_text_cache'):
            self._text_cache = {}
        if len(self._text_cache) < 1000:  # Limite du cache
            self._text_cache[text_hash] = cleaned
        
        return cleaned
    
    def _extract_required_keywords(self, content: str, query_words: List[str]) -> List[List[Any]]:
        """Extrait les mots-cl√©s obligatoires avec leurs statistiques"""
        # Utiliser le mode inclusif pour capturer tous les mots
        words = self._tokenize_and_filter(content, include_short_words=True)
        word_freq = Counter(words)
        
        # Calcul TF-IDF pour l'importance
        tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words=list(self.french_stopwords))
        try:
            tfidf_matrix = tfidf_vectorizer.fit_transform([content])
            feature_names = tfidf_vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]
            tfidf_dict = dict(zip(feature_names, tfidf_scores))
        except:
            tfidf_dict = {}
        
        # S√©lection des mots-cl√©s obligatoires
        keywords = []
        
        # 1. D'abord, forcer l'inclusion des mots de la requ√™te
        for query_word in query_words:
            if query_word in word_freq:
                freq = word_freq[query_word]
                tfidf_score = tfidf_dict.get(query_word, 0)
                importance = int(tfidf_score * 100) if tfidf_score > 0 else freq
                importance += 30  # Bonus important pour les mots de la requ√™te
                keywords.append([query_word, freq, importance])
                print(f"üéØ Mot de requ√™te ajout√©: {query_word} (fr√©q: {freq}, importance: {importance})")
        
        # 2. Ensuite, ajouter les autres mots-cl√©s
        for word, freq in word_freq.most_common(50):
            # Skip si d√©j√† ajout√© comme mot de requ√™te
            if word in query_words:
                continue
                
            if len(word) > 2 and freq > 1:  # Garder 3+ caract√®res pour les autres mots
                tfidf_score = tfidf_dict.get(word, 0)
                importance = int(tfidf_score * 100) if tfidf_score > 0 else freq
                keywords.append([word, freq, importance])
        
        # Trie par importance d√©croissante
        keywords.sort(key=lambda x: x[2], reverse=True)
        return keywords[:45]  # Top 45 comme dans l'exemple
    
    def _tokenize_and_filter(self, text: str, include_short_words: bool = False) -> List[str]:
        """Tokenise et filtre le texte"""
        clean_text = self._clean_text(text)
        words = word_tokenize(clean_text, language='french')
        
        # Filtre les mots courts et les stop words
        if include_short_words:
            # Mode inclusif pour les mots de la requ√™te
            filtered_words = [
                word for word in words 
                if len(word) > 1 and word not in self.french_stopwords
            ]
        else:
            filtered_words = [
                word for word in words 
                if len(word) > 2 and word not in self.french_stopwords
            ]
        
        return filtered_words
    
    def _extract_complementary_keywords(self, content: str, required_keywords: List[List[Any]]) -> List[List[Any]]:
        """Extrait les mots-cl√©s compl√©mentaires"""
        words = self._tokenize_and_filter(content)
        word_freq = Counter(words)
        
        # Mots d√©j√† utilis√©s dans les obligatoires
        required_words = [kw[0] for kw in required_keywords]
        
        complementary = []
        for word, freq in word_freq.most_common(200):
            if word not in required_words and len(word) > 3:
                # Score bas√© sur la fr√©quence et la longueur
                score = min(freq + len(word) - 3, 33)
                complementary.append([word, freq, score])
        
        # Trie par score d√©croissant
        complementary.sort(key=lambda x: x[2], reverse=True)
        return complementary[:100]  # Top 100 mots compl√©mentaires
    
    def _extract_ngrams(self, content: str, query: str) -> List[List[Any]]:
        """Extrait les n-grammes les plus pertinents avec scores d'importance"""
        clean_text = self._clean_text(content)
        words = clean_text.split()
        query_words = self._clean_text(query).split()
        
        # Extraction des expressions de 4-5 mots (plus longues que bigrams/trigrams)
        ngrams_4 = []
        ngrams_5 = []
        
        # N-grammes de 4 mots
        for i in range(len(words) - 3):
            ngram = f"{words[i]} {words[i+1]} {words[i+2]} {words[i+3]}"
            if len(ngram) > 15 and self._is_valid_ngram(ngram):
                ngrams_4.append(ngram)
        
        # N-grammes de 5 mots
        for i in range(len(words) - 4):
            ngram = f"{words[i]} {words[i+1]} {words[i+2]} {words[i+3]} {words[i+4]}"
            if len(ngram) > 20 and self._is_valid_ngram(ngram):
                ngrams_5.append(ngram)
        
        # Compte les occurrences
        all_ngrams = ngrams_4 + ngrams_5
        ngram_counts = Counter(all_ngrams)
        
        # Cr√©ation de la liste avec scores
        ngram_keywords = []
        
        for ngram, freq in ngram_counts.most_common(100):
            if freq > 1:  # Au moins 2 occurrences
                # Calcul de l'importance
                importance = freq * 4  # Base sur la fr√©quence
                
                # Bonus majeur si contient des mots de la requ√™te
                ngram_words = ngram.split()
                query_match_count = sum(1 for word in ngram_words if word in query_words)
                importance += query_match_count * 25
                
                # Bonus pour expressions s√©mantiquement riches
                semantic_words = ['comment', 'pourquoi', 'quand', 'guide', 'conseil', 'astuce', 'm√©thode', 'technique', 'strat√©gie', 'comparaison', 'diff√©rence', 'avantage', 'inconv√©nient', 'bienfait', 'effet', 'r√©sultat']
                if any(word in ngram.lower() for word in semantic_words):
                    importance += 15
                
                # Bonus pour longueur (expressions plus descriptives)
                if len(ngram) > 30:
                    importance += 10
                
                ngram_keywords.append([ngram, freq, importance])
        
        # Trie par importance d√©croissante
        ngram_keywords.sort(key=lambda x: x[2], reverse=True)
        
        return ngram_keywords[:25]  # Top 25 n-grammes
    
    def _is_valid_ngram(self, ngram: str) -> bool:
        """Valide si un n-gramme long est pertinent"""
        words = ngram.split()
        
        # Doit avoir au moins 4 mots
        if len(words) < 4:
            return False
        
        # Mots vides basiques √† √©viter
        stop_words = {'de', 'du', 'des', 'le', 'la', 'les', 'un', 'une', 'et', 'ou', '√†', 'au', 'aux', 'en'}
        
        # Ne doit pas commencer ou finir par un mot vide
        if words[0] in stop_words or words[-1] in stop_words:
            return False
        
        # √âvite les n-grammes avec trop de mots vides (max 30%)
        stop_word_count = sum(1 for word in words if word in stop_words)
        if stop_word_count / len(words) > 0.3:
            return False
        
        # √âvite les patterns trop r√©p√©titifs
        unique_words = set(words)
        if len(unique_words) < len(words) * 0.7:  # Au moins 70% de mots uniques
            return False
        
        return True
    
    def _extract_bigrams(self, content: str, query: str) -> List[List[Any]]:
        """Extrait les groupes de mots-cl√©s de 2 mots avec analyse de leur importance - Version optimis√©e"""
        clean_text = self._clean_text(content)
        words = clean_text.split()
        query_words = self._clean_text(query).split()
        
        # Extraction optimis√©e avec pr√©-filtrage
        bigrams = [f"{words[i]} {words[i+1]}" for i in range(len(words) - 1) 
                  if len(f"{words[i]} {words[i+1]}") > 6]
        
        # Comptage rapide des occurrences
        bigram_counts = Counter(bigrams)
        
        # Cache des mots SEO pour √©viter les recherches r√©p√©t√©es
        seo_words_set = frozenset(['seo', 'r√©f√©rencement', 'google', 'naturel', 'optimisation', 'ranking'])
        query_words_set = frozenset(query_words)
        
        # Traitement optimis√© avec pr√©-filtrage
        bigram_keywords = []
        filtered_count = 0
        
        for bigram, freq in bigram_counts.most_common(200):
            if freq > 1 and self._is_valid_bigram(bigram):
                # Calcul optimis√© de l'importance
                importance = freq * 2
                
                # Bonus optimis√© pour mots de requ√™te (intersection de sets)
                bigram_words_set = frozenset(bigram.split())
                if bigram_words_set & query_words_set:
                    importance += 15
                
                # Bonus SEO optimis√©
                if any(seo_word in bigram.lower() for seo_word in seo_words_set):
                    importance += 10
                
                bigram_keywords.append([bigram, freq, importance])
            else:
                filtered_count += 1
        
        # Tri par importance d√©croissante
        bigram_keywords.sort(key=lambda x: x[2], reverse=True)
        
        print(f"üîç Bigrams: {len(bigram_keywords)} gard√©s, {filtered_count} filtr√©s sur {len(bigram_counts)} analys√©s")
        
        return bigram_keywords[:25]  # Top 25 bigrams
    
    def _extract_trigrams(self, content: str, query: str) -> List[List[Any]]:
        """Extrait les groupes de mots-cl√©s de 3 mots avec analyse de leur importance - Version optimis√©e"""
        clean_text = self._clean_text(content)
        words = clean_text.split()
        query_words = self._clean_text(query).split()
        
        # Extraction optimis√©e avec pr√©-filtrage
        trigrams = [f"{words[i]} {words[i+1]} {words[i+2]}" for i in range(len(words) - 2) 
                   if len(f"{words[i]} {words[i+1]} {words[i+2]}") > 10]
        
        # Comptage rapide des occurrences
        trigram_counts = Counter(trigrams)
        
        # Cache des mots SEO et requ√™te pour optimisation
        seo_words_set = frozenset(['seo', 'r√©f√©rencement', 'google', 'naturel', 'optimisation', 'ranking'])
        query_words_set = frozenset(query_words)
        
        # Traitement optimis√©
        trigram_keywords = []
        filtered_count = 0
        
        for trigram, freq in trigram_counts.most_common(150):
            if freq > 1 and self._is_valid_trigram(trigram):
                # Calcul optimis√© de l'importance
                importance = freq * 3
                
                # Bonus optimis√© pour mots de requ√™te (intersection de sets)
                trigram_words_set = frozenset(trigram.split())
                if trigram_words_set & query_words_set:
                    importance += 20
                
                # Bonus SEO optimis√©
                if any(seo_word in trigram.lower() for seo_word in seo_words_set):
                    importance += 15
                
                # Bonus longueur optimis√© (√©vite len() r√©p√©t√©)
                if len(trigram) > 20:
                    importance += 5
                
                trigram_keywords.append([trigram, freq, importance])
            else:
                filtered_count += 1
        
        # Tri par importance d√©croissante
        trigram_keywords.sort(key=lambda x: x[2], reverse=True)
        
        print(f"üîç Trigrams: {len(trigram_keywords)} gard√©s, {filtered_count} filtr√©s sur {len(trigram_counts)} analys√©s")
        
        return trigram_keywords[:20]  # Top 20 trigrams
    
    def _is_valid_bigram(self, bigram: str) -> bool:
        """Valide si un bigram est un vrai groupe de mots-cl√©s - Version optimis√©e"""
        words = bigram.split()
        if len(words) != 2:
            return False
        
        # Check direct dans les patterns invalides (plus rapide)
        if bigram in self.invalid_bigram_patterns:
            return False
        
        # √âvite les bigrams commen√ßant ou finissant par des mots vides (utilise le cache)
        if words[0] in self.validation_stop_words or words[1] in self.validation_stop_words:
            return False
        
        # V√©rification rapide des mots trop courts (utilise le cache SEO)
        for word in words:
            if len(word) < 3 and word.lower() not in self.seo_exceptions:
                return False
        
        return True
    
    def _is_valid_trigram(self, trigram: str) -> bool:
        """Valide si un trigram est une vraie expression de mots-cl√©s - Version optimis√©e"""
        words = trigram.split()
        if len(words) != 3:
            return False
        
        # √âvite les trigrams commen√ßant ou finissant par des mots vides (cache)
        if words[0] in self.validation_stop_words or words[2] in self.validation_stop_words:
            return False
        
        # Autorise un seul mot vide au milieu (ex: "agence de communication")
        stop_count = sum(1 for word in words if word in self.validation_stop_words)
        if stop_count > 1:
            return False
        
        # Si un mot vide est pr√©sent, il doit √™tre au milieu
        if stop_count == 1 and words[1] not in self.validation_stop_words:
            return False
        
        # V√©rification rapide des mots trop courts (cache SEO)
        for word in words:
            if len(word) < 3 and word.lower() not in self.seo_exceptions and word not in self.validation_stop_words:
                return False
        
        # Check rapide des patterns invalides (cache)
        bigram_start = f"{words[0]} {words[1]}"
        bigram_end = f"{words[1]} {words[2]}"
        
        if bigram_start in self.invalid_trigram_starts or bigram_end in self.invalid_trigram_ends:
            return False
        
        return True
    
    def _add_minmax_stats(self, keywords: List[List[Any]], organic_results: List[Dict[str, Any]]) -> List[List[Any]]:
        """Ajoute les statistiques min-max d'occurrences pour chaque mot-cl√© - Version optimis√©e"""
        enhanced_keywords = []
        
        # Cache pour √©viter de retokeniser le m√™me contenu plusieurs fois
        content_cache = {}
        
        for keyword_info in keywords:
            keyword = keyword_info[0]
            freq = keyword_info[1]
            importance = keyword_info[2]
            keyword_lower = keyword.lower()
            
            # Analyser les occurrences dans chaque page concurrente
            occurrences = []
            
            for i, result in enumerate(organic_results):
                # Utilise le cache pour √©viter retokenisation
                if i not in content_cache:
                    content = result.get("content", "") + " " + result.get("title", "") + " " + result.get("h1", "") + " " + result.get("h2", "") + " " + result.get("h3", "")
                    content_cache[i] = self._tokenize_and_filter(content.lower(), include_short_words=True)
                
                count = content_cache[i].count(keyword_lower)
                if count > 0:  # Ne compter que les pages qui utilisent le mot-cl√©
                    occurrences.append(count)
            
            if occurrences:
                min_occ = min(occurrences)
                max_occ = max(occurrences)
            else:
                # Valeurs par d√©faut bas√©es sur la fr√©quence globale
                min_occ = max(1, freq // 3)
                max_occ = freq * 2
            
            # Format : [mot-cl√©, fr√©quence, importance, min_occurrences, max_occurrences]
            enhanced_keywords.append([keyword, freq, importance, min_occ, max_occ])
        
        return enhanced_keywords
    
    def _generate_questions(self, query: str, keywords: List[List[Any]], paa_questions: List[str] = None) -> str:
        """G√©n√®re des questions pertinentes bas√©es sur la requ√™te, les mots-cl√©s et les PAA"""
        questions = []
        
        # Ajout des questions PAA en priorit√© (donn√©es r√©elles de Google)
        if paa_questions:
            questions.extend(paa_questions)
        
        # Questions g√©n√©r√©es automatiquement
        auto_questions = [
            f"Qu'est-ce que {query} ?",
            f"Comment choisir {query} ?",
            f"Pourquoi utiliser {query} ?",
            f"Quand prendre {query} ?",
            f"Quel est le meilleur {query} ?",
            f"Comment fonctionne {query} ?",
            f"Quels sont les bienfaits de {query} ?",
            f"Quelle est la diff√©rence entre {query} ?",
            f"Comment prendre {query} ?",
            f"Faut-il prendre {query} ?"
        ]
        questions.extend(auto_questions)
        
        # Questions bas√©es sur les mots-cl√©s principaux
        top_keywords = [kw[0] for kw in keywords[:5]]
        for keyword in top_keywords:
            questions.extend([
                f"Pourquoi {keyword} est important ?",
                f"Comment {keyword} fonctionne ?",
                f"Quel est l'effet de {keyword} ?",
                f"Quand utiliser {keyword} ?"
            ])
        
        return ";".join(questions[:60])
    
    def _analyze_competitors(self, serp_results: List[Dict[str, Any]], keywords: List[List[Any]]) -> List[Dict[str, Any]]:
        """
        üîç ANALYSE D√âTAILL√âE DE CHAQUE CONCURRENT AVEC SEUILS ADAPTATIFS
        
        üìä M√©triques calcul√©es par concurrent :
        - Score SEO global
        - Niveau de suroptimisation d√©taill√© avec seuils bas√©s sur la concurrence
        - Analyse des densit√©s de mots-cl√©s (normes du march√©)
        - Patterns de keyword stuffing d√©tect√©s
        - Recommandations d'optimisation adaptatives
        """
        # √âTAPE 1: Collecte des donn√©es pour √©tablir les normes du march√©
        market_data = self._analyze_market_norms(serp_results, keywords)
        
        competitors = []
        keyword_dict = {kw[0]: kw[1] for kw in keywords}
        
        for result in serp_results:
            if not result.get("url"):
                continue
                
            # Contenu complet pour analyse
            full_content = " ".join([
                result.get("title", ""),
                result.get("h1", ""),
                result.get("h2", ""),
                result.get("h3", ""),
                result.get("content", ""),
                result.get("snippet", "")
            ])
            
            # Calculs principaux avec seuils adaptatifs
            score = self._calculate_seo_score(full_content, keyword_dict)
            suroptimisation = self._calculate_adaptive_overoptimization(full_content, keywords, market_data)
            
            # üî¨ ANALYSE D√âTAILL√âE DE SUROPTIMISATION ADAPTATIVE
            overopt_details = self._analyze_competitor_overoptimization_adaptive(full_content, keywords, market_data)
            
            competitor = {
                "h1": result.get("h1", ""),
                "title": result.get("title", ""),
                "h2": result.get("h2", ""),
                "h3": result.get("h3", ""),
                "score": score,
                "suroptimisation": suroptimisation,
                "position": result.get("position", 0),
                "words": result.get("word_count", 0),
                "url": result.get("url", ""),
                "domaine": result.get("domain", ""),
                "internal_links": result.get("internal_links", 0),
                "external_links": result.get("external_links", 0),
                "tableaux": result.get("tables", 0),
                "titles": result.get("titles", 0),
                "video": result.get("videos", 0),
                "liste": result.get("lists", 0),
                "image": result.get("images", 0),
                
                # üÜï NOUVELLES M√âTRIQUES DE SUROPTIMISATION ADAPTATIVES
                "overopt_details": overopt_details,
                "keyword_density_total": overopt_details.get("total_density", 0),
                "stuffing_patterns": overopt_details.get("stuffing_count", 0),
                "clustering_issues": overopt_details.get("clustering_penalty", 0),
                "overopt_level": suroptimisation,
                "recommendations": self._generate_adaptive_optimization_recommendations(overopt_details, market_data)
            }
            
            competitors.append(competitor)
        
        # Tri par position pour faciliter l'analyse
        competitors.sort(key=lambda x: x.get("position", 999))
        
        return competitors
    
    def _analyze_market_norms(self, serp_results: List[Dict[str, Any]], keywords: List[List[Any]]) -> Dict[str, Any]:
        """
        üìä ANALYSE DES NORMES DU MARCH√â POUR √âTABLIR DES SEUILS ADAPTATIFS
        
        Collecte et analyse les densit√©s/fr√©quences de tous les concurrents
        pour √©tablir des seuils r√©alistes bas√©s sur les donn√©es r√©elles.
        """
        market_densities = {}  # {keyword: [density1, density2, ...]}
        market_frequencies = {}  # {keyword: [freq1, freq2, ...]}
        total_densities = []  # Densit√©s totales de chaque concurrent
        
        for result in serp_results:
            if not result.get("url"):
                continue
                
            full_content = " ".join([
                result.get("title", ""),
                result.get("h1", ""),
                result.get("h2", ""),
                result.get("h3", ""),
                result.get("content", ""),
                result.get("snippet", "")
            ])
            
            if not full_content.strip():
                continue
                
            content_words = self._tokenize_and_filter(full_content)
            word_counts = Counter(content_words)
            total_words = len(content_words)
            
            if total_words == 0:
                continue
            
            competitor_total_density = 0
            
            # Analyse des 15 mots-cl√©s principaux
            for keyword_info in keywords[:15]:
                keyword = keyword_info[0].lower()
                frequency = word_counts.get(keyword, 0)
                density = (frequency / total_words) * 100 if frequency > 0 else 0
                
                if keyword not in market_densities:
                    market_densities[keyword] = []
                    market_frequencies[keyword] = []
                
                market_densities[keyword].append(density)
                market_frequencies[keyword].append(frequency)
                competitor_total_density += density
            
            total_densities.append(competitor_total_density)
        
        # Calcul des seuils adaptatifs bas√©s sur les percentiles
        adaptive_thresholds = {}
        
        for keyword in market_densities:
            densities = [d for d in market_densities[keyword] if d > 0]
            frequencies = [f for f in market_frequencies[keyword] if f > 0]
            
            if densities:
                # Tri pour calculs de percentiles et m√©diane
                sorted_densities = sorted(densities)
                sorted_frequencies = sorted(frequencies)
                
                # Calculs de base
                mean_density = sum(densities) / len(densities)
                min_density = min(densities) 
                max_density = max(densities)
                median_density = sorted_densities[len(sorted_densities) // 2]
                
                mean_frequency = sum(frequencies) / len(frequencies)
                min_frequency = min(frequencies)
                max_frequency = max(frequencies)
                median_frequency = sorted_frequencies[len(sorted_frequencies) // 2]
                
                # Percentiles pour r√©trocompatibilit√©
                p75_density = sorted_densities[int(len(sorted_densities) * 0.75)] if len(sorted_densities) > 3 else max_density
                p90_density = sorted_densities[int(len(sorted_densities) * 0.90)] if len(sorted_densities) > 9 else max_density
                p75_frequency = sorted_frequencies[int(len(sorted_frequencies) * 0.75)] if len(sorted_frequencies) > 3 else max_frequency
                p90_frequency = sorted_frequencies[int(len(sorted_frequencies) * 0.90)] if len(sorted_frequencies) > 9 else max_frequency
                
                adaptive_thresholds[keyword] = {
                    # Anciennes m√©triques pour r√©trocompatibilit√©
                    'density_moderate': max(p75_density * 1.3, mean_density + 1.0),
                    'density_high': max(p90_density * 1.2, mean_density + 2.0),
                    'density_critical': max(max_density * 1.1, mean_density + 3.0),
                    'frequency_moderate': max(p75_frequency * 1.5, mean_frequency + 5),
                    'frequency_high': max(p90_frequency * 1.3, mean_frequency + 10),
                    'frequency_critical': max(max_frequency * 1.2, mean_frequency + 15),
                    
                    # Nouvelles m√©triques pour le syst√®me bas√© sur la m√©diane
                    'market_min_density': min_density,
                    'market_max_density': max_density,
                    'market_median_density': median_density,
                    'market_mean_density': mean_density,
                    'market_min_frequency': min_frequency,
                    'market_max_frequency': max_frequency,
                    'market_median_frequency': median_frequency,
                    'market_mean_frequency': mean_frequency
                }
        
        # Seuils pour la densit√© totale
        if total_densities:
            mean_total = sum(total_densities) / len(total_densities)
            max_total = max(total_densities)
            p75_total = sorted(total_densities)[int(len(total_densities) * 0.75)] if len(total_densities) > 3 else max_total
            p90_total = sorted(total_densities)[int(len(total_densities) * 0.90)] if len(total_densities) > 9 else max_total
            
            total_thresholds = {
                'moderate': max(p75_total * 1.4, mean_total + 5),
                'high': max(p90_total * 1.3, mean_total + 8),
                'critical': max(max_total * 1.2, mean_total + 12),
                'market_mean': mean_total,
                'market_max': max_total
            }
        else:
            # Fallback si pas de donn√©es
            total_thresholds = {
                'moderate': 20, 'high': 30, 'critical': 40,
                'market_mean': 10, 'market_max': 25
            }
        
        return {
            'keyword_thresholds': adaptive_thresholds,
            'total_density_thresholds': total_thresholds,
            'competitors_analyzed': len([r for r in serp_results if r.get("url")])
        }
    
    def _analyze_competitor_overoptimization(self, content: str, keywords: List[List[Any]]) -> Dict[str, Any]:
        """Analyse d√©taill√©e de la suroptimisation d'un concurrent"""
        if not content:
            return {"total_density": 0, "stuffing_count": 0, "clustering_penalty": 0, "flagged_keywords": []}
        
        content_words = self._tokenize_and_filter(content)
        content_lower = content.lower()
        word_counts = Counter(content_words)
        total_words = len(content_words)
        
        if total_words == 0:
            return {"total_density": 0, "stuffing_count": 0, "clustering_penalty": 0, "flagged_keywords": []}
        
        total_density = 0
        flagged_keywords = []
        stuffing_count = 0
        total_clustering_penalty = 0
        
        # Analyse de chaque mot-cl√© top 10
        for keyword_info in keywords[:10]:
            keyword = keyword_info[0].lower()
            frequency = word_counts.get(keyword, 0)
            
            if frequency == 0:
                continue
            
            density = (frequency / total_words) * 100
            total_density += density
            
            # Identifier les mots-cl√©s probl√©matiques
            keyword_analysis = {
                "keyword": keyword,
                "frequency": frequency,
                "density": round(density, 2),
                "issues": []
            }
            
            # D√©tection des probl√®mes - Seuils ajust√©s
            if density > 4.5:
                keyword_analysis["issues"].append("Densit√© critique (>4.5%)")
            elif density > 3.0:
                keyword_analysis["issues"].append("Densit√© √©lev√©e (>3.0%)")
            
            if frequency > 20:
                keyword_analysis["issues"].append("Fr√©quence excessive (>20)")
                stuffing_count += 1
            
            # Clustering
            positions = []
            start = 0
            while True:
                pos = content_lower.find(keyword, start)
                if pos == -1:
                    break
                positions.append(pos)
                start = pos + 1
                
            if len(positions) >= 3:
                clustering_penalty = self._detect_keyword_clustering(positions, len(content))
                if clustering_penalty > 0:
                    keyword_analysis["issues"].append(f"Clustering d√©tect√© (p√©nalit√©: {clustering_penalty})")
                    total_clustering_penalty += clustering_penalty
            
            # Patterns de stuffing
            double_pattern = f"{keyword} {keyword}"
            if double_pattern in content_lower:
                keyword_analysis["issues"].append("R√©p√©tition imm√©diate d√©tect√©e")
                stuffing_count += 1
            
            comma_pattern = f"{keyword},"
            if content_lower.count(comma_pattern) >= 2:
                keyword_analysis["issues"].append("Pattern de liste d√©tect√©")
                stuffing_count += 1
            
            if keyword_analysis["issues"]:
                flagged_keywords.append(keyword_analysis)
        
        return {
            "total_density": round(total_density, 2),
            "stuffing_count": stuffing_count,
            "clustering_penalty": total_clustering_penalty,
            "flagged_keywords": flagged_keywords,
            "content_length": total_words
        }
    
    def _analyze_competitor_overoptimization_adaptive(self, content: str, keywords: List[List[Any]], market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyse d√©taill√©e de la suroptimisation avec seuils adaptatifs bas√©s sur la concurrence"""
        if not content:
            return {"total_density": 0, "stuffing_count": 0, "clustering_penalty": 0, "flagged_keywords": []}
        
        content_words = self._tokenize_and_filter(content)
        content_lower = content.lower()
        word_counts = Counter(content_words)
        total_words = len(content_words)
        
        if total_words == 0:
            return {"total_density": 0, "stuffing_count": 0, "clustering_penalty": 0, "flagged_keywords": []}
        
        total_density = 0
        flagged_keywords = []
        stuffing_count = 0
        total_clustering_penalty = 0
        keyword_thresholds = market_data.get('keyword_thresholds', {})
        
        # Analyse de chaque mot-cl√© avec seuils adaptatifs
        for keyword_info in keywords[:10]:
            keyword = keyword_info[0].lower()
            frequency = word_counts.get(keyword, 0)
            
            if frequency == 0:
                continue
            
            density = (frequency / total_words) * 100
            total_density += density
            
            # R√©cup√©ration des seuils adaptatifs pour ce mot-cl√©
            thresholds = keyword_thresholds.get(keyword, {
                'density_moderate': 2.0, 'density_high': 3.0, 'density_critical': 4.5,
                'frequency_moderate': 15, 'frequency_high': 25, 'frequency_critical': 35
            })
            
            # Identifier les mots-cl√©s probl√©matiques avec seuils adaptatifs
            keyword_analysis = {
                "keyword": keyword,
                "frequency": frequency,
                "density": round(density, 2),
                "issues": [],
                "market_context": {
                    "mean_density": round(thresholds.get('market_mean_density', 0), 2),
                    "max_density": round(thresholds.get('market_max_density', 0), 2),
                    "mean_frequency": round(thresholds.get('market_mean_frequency', 0), 1),
                    "max_frequency": thresholds.get('market_max_frequency', 0)
                }
            }
            
            # D√©tection des probl√®mes avec seuils adaptatifs
            if density > thresholds['density_critical']:
                keyword_analysis["issues"].append(f"Densit√© critique (>{thresholds['density_critical']:.1f}% vs march√© max: {thresholds.get('market_max_density', 0):.1f}%)")
            elif density > thresholds['density_high']:
                keyword_analysis["issues"].append(f"Densit√© √©lev√©e (>{thresholds['density_high']:.1f}% vs march√© moy: {thresholds.get('market_mean_density', 0):.1f}%)")
            
            if frequency > thresholds['frequency_critical']:
                keyword_analysis["issues"].append(f"Fr√©quence critique (>{thresholds['frequency_critical']} vs march√© max: {thresholds.get('market_max_frequency', 0)})")
                stuffing_count += 1
            elif frequency > thresholds['frequency_high']:
                keyword_analysis["issues"].append(f"Fr√©quence √©lev√©e (>{thresholds['frequency_high']} vs march√© moy: {thresholds.get('market_mean_frequency', 0):.0f})")
            
            # Clustering (logique inchang√©e)
            positions = []
            start = 0
            while True:
                pos = content_lower.find(keyword, start)
                if pos == -1:
                    break
                positions.append(pos)
                start = pos + 1
                
            if len(positions) >= 3:
                clustering_penalty = self._detect_keyword_clustering(positions, len(content))
                if clustering_penalty > 0:
                    keyword_analysis["issues"].append(f"Clustering d√©tect√© (p√©nalit√©: {clustering_penalty})")
                    total_clustering_penalty += clustering_penalty
            
            # Patterns de stuffing (logique inchang√©e)
            double_pattern = f"{keyword} {keyword}"
            if double_pattern in content_lower:
                keyword_analysis["issues"].append("R√©p√©tition imm√©diate d√©tect√©e")
                stuffing_count += 1
            
            comma_pattern = f"{keyword},"
            if content_lower.count(comma_pattern) >= 2:
                keyword_analysis["issues"].append("Pattern de liste d√©tect√©")
                stuffing_count += 1
            
            if keyword_analysis["issues"]:
                flagged_keywords.append(keyword_analysis)
        
        return {
            "total_density": round(total_density, 2),
            "stuffing_count": stuffing_count,
            "clustering_penalty": total_clustering_penalty,
            "flagged_keywords": flagged_keywords,
            "content_length": total_words,
            "market_context": market_data.get('total_density_thresholds', {})
        }
    
    def _classify_overoptimization_level(self, score: int) -> str:
        """Classifie le niveau de suroptimisation avec des seuils simples"""
        if score >= 50:
            return "üíÄ Extr√™me"
        elif score >= 35:
            return "üö® Critique"
        elif score >= 25:
            return "‚ö†Ô∏è √âlev√©"
        elif score >= 15:
            return "‚ö° Mod√©r√©"
        elif score >= 8:
            return "üìä Faible"
        else:
            return "‚úÖ Optimal"
    
    def _generate_optimization_recommendations(self, overopt_details: Dict[str, Any]) -> List[str]:
        """G√©n√®re des recommandations d'optimisation"""
        recommendations = []
        
        total_density = overopt_details.get("total_density", 0)
        stuffing_count = overopt_details.get("stuffing_count", 0)
        clustering_penalty = overopt_details.get("clustering_penalty", 0)
        flagged_keywords = overopt_details.get("flagged_keywords", [])
        
        if total_density > 25:
            recommendations.append("R√©duire la densit√© totale des mots-cl√©s (<18%)")
        
        if stuffing_count > 0:
            recommendations.append("√âliminer les patterns de keyword stuffing")
        
        if clustering_penalty > 15:
            recommendations.append("Distribuer les mots-cl√©s plus naturellement")
        
        for kw in flagged_keywords:
            if kw["density"] > 4.5:
                recommendations.append(f"R√©duire la densit√© de '{kw['keyword']}' (<3%)")
        
        if not recommendations:
            recommendations.append("Optimisation √©quilibr√©e - aucun probl√®me majeur d√©tect√©")
        
        return recommendations
    
    def _calculate_adaptive_overoptimization(self, content: str, keywords: List[List[Any]], market_data: Dict[str, Any]) -> int:
        """
        Calcule le score de suroptimisation bas√© sur l'√©cart par rapport √† la m√©diane du march√©
        
        üéØ NOUVELLE APPROCHE :
        - Score 0 = Dans la norme (‚â§ m√©diane)
        - Score 1-50 = Au-dessus de la m√©diane, approche progressive
        - Score 51-100 = Spam √©vident, tr√®s au-dessus du maximum du march√©
        """
        if not content:
            return 0
            
        content_words = self._tokenize_and_filter(content)
        word_counts = Counter(content_words)
        total_words = len(content_words)
        
        if total_words == 0:
            return 0
        
        total_score = 0
        keyword_thresholds = market_data.get('keyword_thresholds', {})
        
        # Analyse des 10 mots-cl√©s principaux
        for keyword_info in keywords[:10]:
            keyword = keyword_info[0].lower()
            frequency = word_counts.get(keyword, 0)
            
            if frequency == 0:
                continue
                
            density = (frequency / total_words) * 100
            
            # R√©cup√©ration des stats du march√© pour ce mot-cl√©
            market_stats = keyword_thresholds.get(keyword, {})
            market_min_density = market_stats.get('market_min_density', 0)
            market_max_density = market_stats.get('market_max_density', 5)
            market_median_density = market_stats.get('market_median_density', 1)
            
            market_min_freq = market_stats.get('market_min_frequency', 0)
            market_max_freq = market_stats.get('market_max_frequency', 20)
            market_median_freq = market_stats.get('market_median_frequency', 3)
            
            # Score bas√© sur la densit√©
            if density > market_median_density:
                if density > market_max_density:
                    # Au-dessus du maximum du march√© = spam √©vident
                    excess_ratio = (density - market_max_density) / max(market_max_density, 1)
                    density_score = min(50 + int(excess_ratio * 30), 70)  # 50-70 points
                else:
                    # Entre m√©diane et maximum = suroptimisation progressive
                    progress_ratio = (density - market_median_density) / max(market_max_density - market_median_density, 0.1)
                    density_score = int(progress_ratio * 30)  # 0-30 points
            else:
                density_score = 0  # Dans la norme
            
            # Score bas√© sur la fr√©quence
            if frequency > market_median_freq:
                if frequency > market_max_freq:
                    # Au-dessus du maximum du march√© = spam √©vident
                    excess_ratio = (frequency - market_max_freq) / max(market_max_freq, 1)
                    freq_score = min(30 + int(excess_ratio * 20), 50)  # 30-50 points
                else:
                    # Entre m√©diane et maximum = suroptimisation progressive
                    progress_ratio = (frequency - market_median_freq) / max(market_max_freq - market_median_freq, 1)
                    freq_score = int(progress_ratio * 20)  # 0-20 points
            else:
                freq_score = 0  # Dans la norme
            
            # Score pour ce mot-cl√© (maximum des deux scores)
            keyword_score = max(density_score, freq_score)
            total_score += keyword_score
        
        # Limitation finale √† 100 points maximum
        return min(total_score, 100)
    
    def _classify_adaptive_overoptimization_level(self, score: int, market_data: Dict[str, Any]) -> str:
        """Classifie le niveau de suroptimisation avec la nouvelle √©chelle 0-100"""
        # Nouvelle √©chelle bas√©e sur la m√©diane du march√©
        if score >= 80:
            return "üíÄ Extr√™me"  # Spam √©vident, tr√®s au-dessus du maximum
        elif score >= 60:
            return "üö® Critique"  # Spam mod√©r√©, au-dessus du maximum
        elif score >= 40:
            return "‚ö†Ô∏è √âlev√©"     # Suroptimisation visible, pr√®s du maximum
        elif score >= 20:
            return "‚ö° Mod√©r√©"     # L√©g√®rement au-dessus de la m√©diane
        elif score >= 10:
            return "üìä Faible"     # Juste au-dessus de la m√©diane
        else:
            return "‚úÖ Optimal"    # Dans la norme du march√© (‚â§ m√©diane)
    
    def _generate_adaptive_optimization_recommendations(self, overopt_details: Dict[str, Any], market_data: Dict[str, Any]) -> List[str]:
        """G√©n√®re des recommandations d'optimisation adaptatives bas√©es sur l'analyse concurrentielle"""
        recommendations = []
        
        total_density = overopt_details.get("total_density", 0)
        stuffing_count = overopt_details.get("stuffing_count", 0)
        clustering_penalty = overopt_details.get("clustering_penalty", 0)
        flagged_keywords = overopt_details.get("flagged_keywords", [])
        total_thresholds = market_data.get('total_density_thresholds', {})
        
        # Recommandations bas√©es sur les normes du march√©
        market_mean = total_thresholds.get('market_mean', 10)
        market_max = total_thresholds.get('market_max', 25)
        
        if total_density > total_thresholds.get('critical', 40):
            recommendations.append(f"R√©duire drastiquement la densit√© totale (<{total_thresholds.get('high', 30):.0f}% - march√© moy: {market_mean:.1f}%)")
        elif total_density > total_thresholds.get('high', 30):
            recommendations.append(f"R√©duire la densit√© totale (<{total_thresholds.get('moderate', 20):.0f}% - march√© moy: {market_mean:.1f}%)")
        
        if stuffing_count > 0:
            recommendations.append("√âliminer les patterns de keyword stuffing d√©tect√©s")
        
        if clustering_penalty > 15:
            recommendations.append("Distribuer les mots-cl√©s plus naturellement dans le texte")
        
        # Recommandations sp√©cifiques par mot-cl√© avec contexte march√©
        for kw in flagged_keywords[:3]:  # Limiter aux 3 plus probl√©matiques
            market_context = kw.get("market_context", {})
            if kw["density"] > 4.0:
                target_density = min(market_context.get("mean_density", 2.0) * 1.5, 3.0)
                recommendations.append(f"'{kw['keyword']}': r√©duire √† ~{target_density:.1f}% (march√©: {market_context.get('mean_density', 0):.1f}%)")
        
        if not recommendations:
            if total_density < market_mean * 0.8:
                recommendations.append(f"Optimisation sous-exploit√©e - densit√© actuelle ({total_density:.1f}%) vs march√© ({market_mean:.1f}%)")
            else:
                recommendations.append("Optimisation √©quilibr√©e selon les standards du march√©")
        
        return recommendations
    
    def _calculate_seo_score(self, content: str, keyword_dict: Dict[str, int]) -> int:
        """
        Calcule le score SEO privil√©giant la diversit√© s√©mantique over bourrage
        
        üéØ NOUVELLE APPROCHE :
        - R√©compense la pr√©sence de BEAUCOUP de mots-cl√©s diff√©rents 
        - P√©nalise la concentration excessive sur quelques mots
        - Favorise la densit√© mod√©r√©e plut√¥t qu'√©lev√©e
        - Score r√©aliste entre 15-85 (plus jamais 100 partout)
        """
        if not content:
            return 5
            
        content_words = self._tokenize_and_filter(content)
        word_counts = Counter(content_words)
        total_words = len(content_words)
        
        if total_words == 0:
            return 5
        
        # Analyse des mots-cl√©s par cat√©gories
        primary_keywords = list(keyword_dict.items())[:5]     # Top 5 - obligatoires
        secondary_keywords = list(keyword_dict.items())[5:15] # 5-15 - importants  
        tertiary_keywords = list(keyword_dict.items())[15:30] # 15-30 - diversit√©
        
        total_score = 0
        
        # === SECTION 1: MOTS-CL√âS PRIMAIRES (max 25 points) ===
        primary_score = 0
        for keyword, expected_freq in primary_keywords:
            actual_freq = word_counts.get(keyword, 0)
            if actual_freq > 0:
                density = (actual_freq / total_words) * 100
                
                # Courbe optimale : r√©compense la mod√©ration
                if 0.3 <= density <= 1.5:
                    primary_score += 5  # Zone optimale
                elif 0.1 <= density < 0.3:
                    primary_score += 3  # Sous-optimis√©
                elif 1.5 < density <= 3.0:
                    primary_score += 2  # L√©g√®rement trop
                elif density > 3.0:
                    primary_score += 1  # P√©nalit√© bourrage
                else:
                    primary_score += 1  # Pr√©sence minimale
        
        total_score += min(primary_score, 25)
        
        # === SECTION 2: DIVERSIT√â SECONDAIRE (max 20 points) ===
        secondary_present = sum(1 for kw, _ in secondary_keywords if word_counts.get(kw, 0) > 0)
        # Bonus progressif pour la diversit√©
        if secondary_present >= 8:
            total_score += 20  # Excellente diversit√©
        elif secondary_present >= 6:
            total_score += 15  # Bonne diversit√©
        elif secondary_present >= 4:
            total_score += 10  # Diversit√© correcte
        elif secondary_present >= 2:
            total_score += 5   # Diversit√© faible
        
        # === SECTION 3: RICHESSE S√âMANTIQUE (max 15 points) ===
        tertiary_present = sum(1 for kw, _ in tertiary_keywords if word_counts.get(kw, 0) > 0)
        # Bonus pour le vocabulaire √©tendu
        if tertiary_present >= 10:
            total_score += 15  # Vocabulaire tr√®s riche
        elif tertiary_present >= 7:
            total_score += 10  # Vocabulaire riche
        elif tertiary_present >= 4:
            total_score += 5   # Vocabulaire correct
        
        # === SECTION 4: √âQUILIBRE ET NATUREL (max 15 points) ===
        equilibrium_score = 0
        
        # Bonus pour distribution √©quilibr√©e des densit√©s
        all_densities = []
        for keyword, _ in primary_keywords + secondary_keywords:
            freq = word_counts.get(keyword, 0)
            if freq > 0:
                density = (freq / total_words) * 100
                all_densities.append(density)
        
        if all_densities:
            # Coefficient de variation (√©cart-type / moyenne)
            mean_density = sum(all_densities) / len(all_densities)
            if mean_density > 0:
                variance = sum((d - mean_density) ** 2 for d in all_densities) / len(all_densities)
                cv = (variance ** 0.5) / mean_density
                
                # Bonus pour √©quilibre (coefficient de variation faible)
                if cv < 0.5:
                    equilibrium_score += 10  # Tr√®s √©quilibr√©
                elif cv < 1.0:
                    equilibrium_score += 6   # √âquilibr√©
                elif cv < 1.5:
                    equilibrium_score += 3   # Mod√©r√©ment √©quilibr√©
        
        # Bonus pour longueur de contenu appropri√©e
        if 800 <= total_words <= 2500:
            equilibrium_score += 5  # Longueur optimale
        elif 400 <= total_words < 800 or 2500 < total_words <= 4000:
            equilibrium_score += 2  # Longueur acceptable
        
        total_score += min(equilibrium_score, 15)
        
        # === SECTION 5: P√âNALIT√âS POUR SUROPTIMISATION ===
        # P√©nalit√© pour mots-cl√©s trop concentr√©s
        over_optimized_penalty = 0
        for keyword, _ in primary_keywords:
            freq = word_counts.get(keyword, 0)
            if freq > 0:
                density = (freq / total_words) * 100
                if density > 4.0:
                    over_optimized_penalty += min(int(density - 4.0) * 2, 10)
        
        total_score -= over_optimized_penalty
        
        # Score final entre 5 et 85 (jamais 0 ou 100)
        final_score = max(5, min(total_score, 85))
        
        return final_score
    
    # ANCIENNE FONCTION SUPPRIM√âE - Utilisez _calculate_adaptive_overoptimization √† la place

    def _detect_keyword_clustering(self, positions: List[int], content_length: int) -> int:
        """D√©tecte les clusters anormaux de mots-cl√©s rapproch√©s - Version ultra-r√©duite"""
        if len(positions) < 5:  # Seuil plus √©lev√© : minimum 5 occurrences
            return 0
        
        penalty = 0
        window_size = max(300, content_length // 8)  # Fen√™tre plus large
        
        for i in range(len(positions) - 4):  # Cherche 5+ occurrences minimum
            window_positions = [p for p in positions[i:] if p <= positions[i] + window_size]
            
            if len(window_positions) >= 5:  # 5+ occurrences dans la fen√™tre
                penalty += 1  # P√©nalit√© fixe ultra-r√©duite
                
        return min(penalty, 3)  # Plafond √† 3 points maximum
    
    def _detect_keyword_stuffing_patterns(self, content: str, keywords: List[List[Any]]) -> int:
        """D√©tecte les patterns typiques de keyword stuffing - Version ultra-r√©duite"""
        penalty = 0
        
        # Pattern 1: Listes de mots-cl√©s s√©par√©s par des virgules (plus strict)
        for keyword_info in keywords:
            keyword = keyword_info[0].lower()
            
            # Recherche de patterns comme "cr√©atine, whey, prot√©ine, masse"
            comma_pattern = f"{keyword}," 
            if content.count(comma_pattern) >= 3:  # Seuil plus √©lev√© : 3+ occurrences
                penalty += 1  # P√©nalit√© ultra-r√©duite
            
            # Pattern 2: R√©p√©titions imm√©diates ("cr√©atine cr√©atine cr√©atine")
            double_pattern = f"{keyword} {keyword}"
            if double_pattern in content:
                penalty += 1  # P√©nalit√© ultra-r√©duite
            
            # Pattern 3: Seulement si vraiment excessif
            if content.count(keyword) >= 10:  # Seuil beaucoup plus √©lev√©
                # Compte les occurrences dans un rayon de 100 caract√®res (plus large)
                start = 0
                while True:
                    pos = content.find(keyword, start)
                    if pos == -1:
                        break
                    
                    # V√©rifie la zone autour pour d'autres occurrences
                    zone_start = max(0, pos - 100)  # Rayon plus large
                    zone_end = min(len(content), pos + len(keyword) + 100)
                    zone = content[zone_start:zone_end]
                    
                    zone_count = zone.count(keyword)
                    if zone_count >= 5:  # Seuil plus √©lev√©
                        penalty += 1  # P√©nalit√© ultra-r√©duite
                        break
                    
                    start = pos + 1
        
        return min(penalty, 5)  # Plafond √† 5 points maximum
    
    def _calculate_target_score(self, competitors: List[Dict[str, Any]]) -> int:
        """
        Calcule le score cible recommand√© bas√© sur l'analyse concurrentielle
        
        üìä M√âTHODE DE CALCUL DU SCORE CIBLE :
        
        1Ô∏è‚É£ COLLECTE DES DONN√âES
           - R√©cup√®re les scores SEO de tous les concurrents du top 10
           - Filtre les scores valides (> 0)
        
        2Ô∏è‚É£ ANALYSE COMPETITIVE
           - S√©lectionne les 3 meilleurs scores (top performers)
           - Calcule la moyenne de ces scores d'excellence
        
        3Ô∏è‚É£ OBJECTIF STRAT√âGIQUE 
           - Ajoute 10% √† cette moyenne pour √™tre comp√©titif
           - Assure de surpasser les leaders actuels
        
        4Ô∏è‚É£ VALIDATION
           - Plafonne √† 100 points maximum
           - Score de base : 50 si pas de donn√©es
        
        üí° Le score SEO individuel est calcul√© selon :
        - Densit√© et pr√©sence des mots-cl√©s obligatoires
        - Distribution s√©mantique naturelle
        - Optimisation technique du contenu
        """
        if not competitors:
            return 50
            
        scores = [comp.get("score", 0) for comp in competitors if comp.get("score", 0) > 0]
        if not scores:
            return 50
            
        # Score cible = moyenne des 3 premiers + 5% pour surpasser la concurrence
        top_scores = sorted(scores, reverse=True)[:3]
        target = int(mean(top_scores) + 5)  # Ajout de 5 points au lieu de 10%
        return min(target, 95)  # Plafond plus r√©aliste √† 95
    
    def _calculate_required_words(self, competitors: List[Dict[str, Any]]) -> int:
        """Calcule le nombre de mots recommand√© avec une approche plus √©quilibr√©e"""
        if not competitors:
            return 800
            
        word_counts = [comp.get("words", 0) for comp in competitors if comp.get("words", 0) > 100]
        if not word_counts:
            return 800
        
        # Utiliser la m√©diane plut√¥t que les plus gros pour √©viter les valeurs extr√™mes
        word_counts_sorted = sorted(word_counts)
        median_words = word_counts_sorted[len(word_counts_sorted)//2]
        
        # Cible = m√©diane + marge raisonnable (pas 10% mais +200 mots)
        target = median_words + 200
        
        # Limite minimale raisonnable de 600 mots
        return max(target, 600)
    
    def _calculate_max_overoptimization(self, keywords: List[List[Any]]) -> int:
        """Calcule le seuil maximum de suroptimisation acceptable"""
        if not keywords:
            return 5
            
        # Bas√© sur le nombre de mots-cl√©s principaux
        main_keywords = len([kw for kw in keywords if kw[2] > 15])
        return max(3, min(main_keywords // 2, 8))
    
    def _analyze_content_types(self, serp_results: List[Dict[str, Any]]) -> Dict[str, int]:
        """Analyse les types de contenu dans les r√©sultats"""
        editorial = 0
        catalogue = 0
        fiche_produit = 0
        
        for result in serp_results:
            url = result.get("url", "").lower()
            title = result.get("title", "").lower()
            
            # D√©tection de fiches produits
            if any(term in url for term in ['/produit/', '/product/', 'acheter', 'prix', 'commander']):
                fiche_produit += 1
            # D√©tection de catalogues
            elif any(term in url for term in ['/categorie/', '/collection/', 'boutique', 'shop']):
                catalogue += 1
            # Contenu √©ditorial par d√©faut
            else:
                editorial += 1
        
        total = len(serp_results)
        return {
            "editorial": int((editorial / total) * 100) if total > 0 else 100,
            "catalogue": int((catalogue / total) * 100) if total > 0 else 0,
            "fiche_produit": int((fiche_produit / total) * 100) if total > 0 else 0
        }
    
    def _calculate_word_statistics(self, serp_results: List[Dict[str, Any]]) -> List[int]:
        """Calcule les statistiques de mots (min, max, moyenne)"""
        word_counts = [result.get("word_count", 0) for result in serp_results if result.get("word_count", 0) > 0]
        
        if not word_counts:
            return [800, 1500, 1200]
        
        return [min(word_counts), max(word_counts), int(mean(word_counts))]
    
    def _get_demo_analysis(self, query: str) -> Dict[str, Any]:
        """Retourne l'analyse de d√©monstration bas√©e sur l'exemple fourni"""
        # Calculer un score cible variable selon la requ√™te
        demo_competitors = [
            {"score": 72}, {"score": 64}, {"score": 58}, {"score": 45}, {"score": 38}
        ]
        calculated_target = self._calculate_target_score(demo_competitors)
        
        return {
            "query": query,
            "score_target": calculated_target,
            "mots_requis": 1100,
            "KW_obligatoires": [["cr√©atine",2,44,3,8],["whey",1,35,2,6],["prise",1,33,1,4],["muscle",2,29,2,7],["compl√©ment",2,27,1,5],["masse",2,25,2,6],["bcaa",1,25,1,3],["prot√©ine",5,20,3,12],["alimentaire",2,21,1,4],["musculaire",2,17,2,5],["effet",3,12,1,6],["r√©cup√©ration",1,14,1,3],["musculation",1,12,1,2],["produit",1,12,1,3],["acide",2,10,1,4],["amin√©",2,10,1,4],["force",2,9,1,4],["√©nergie",1,11,1,2],["monohydrate",2,8,1,4],["poudre",2,9,1,4],["dose",1,8,1,2],["consommer",1,8,1,2],["effort",2,9,1,4],["jour",1,8,1,2],["shaker",1,7,1,2],["objectif",1,7,1,2],["source",1,7,1,2],["s√©ance",2,7,1,4],["sport",1,6,1,2],["lait",1,5,1,2],["sportif",1,6,1,2],["endurance",1,5,1,2],["exemple",1,5,1,2],["√©tude",1,5,1,2],["corps",1,6,1,2],["consommation",1,5,1,2],["recommand√©e",2,4,1,4],["bienfait",1,5,1,2],["puissance",1,6,1,2],["apport",1,5,1,2],["graisse",1,6,1,2],["meilleure",1,3,1,2],["haute",1,4,1,2],["sant√©",2,3,1,4],["r√¥le",1,4,1,2]],
            "KW_complementaires": [["pack",2,33,1,4],["collation",2,17,1,3],["taux",5,9,2,8],["substance",2,10,1,3],["point",1,11,1,2],["marque",4,6,2,6],["augmenter",2,8,1,3],["personne",1,8,1,2],["am√©lioration",1,8,1,2],["utilis√©",1,8,1,2],["matin",2,8,1,3],["midi",2,8,1,3],["performance",2,7,1,3],["booster",1,7,1,2],["meilleur",1,7,1,2],["sportive",1,7,1,2],["d√©veloppement",2,5,1,3],["prix",4,5,2,6],["gamme",1,7,1,2],["forme",2,6,1,3],["intense",2,7,1,3],["organisme",1,7,1,2],["court",2,7,1,3],["qualit√©",1,7,1,2],["profiter",1,7,1,2],["risque",1,7,1,2],["composition",2,7,1,3],["fabriqu√©",2,5,1,3],["choisir",2,4,1,3],["associer",2,4,1,3],["nutrition",2,4,1,3],["croissance",1,5,1,2],["isolate",1,4,1,2],["training",2,3,1,3],["eau",2,4,1,3],["b√©n√©fique",1,5,1,2]],
            "ngrams": [
                ["cr√©atine monohydrate pour prise masse", 4, 85],
                ["diff√©rence entre cr√©atine et whey", 3, 82],
                ["comment prendre cr√©atine et whey", 3, 78],
                ["meilleur moment prendre cr√©atine", 2, 65],
                ["suppl√©mentation en cr√©atine et prot√©ine", 2, 62],
                ["effet de la cr√©atine sur", 4, 58],
                ["phase de charge cr√©atine n√©cessaire", 2, 55],
                ["whey protein isolate native qualit√©", 2, 52],
                ["r√©cup√©ration musculaire apr√®s entra√Ænement intensif", 2, 50],
                ["synth√®se des prot√©ines musculaires", 3, 48],
                ["d√©veloppement de la masse musculaire", 2, 45],
                ["ad√©nosine triphosphate et performance sportive", 2, 42],
                ["acides amin√©s essentiels pour muscle", 2, 40],
                ["nutrition sportive optimale pour gains", 2, 38],
                ["compl√©ment alimentaire naturel sans danger", 2, 35],
                ["force et puissance musculaire explosive", 2, 32],
                ["construction musculaire rapide et efficace", 2, 30],
                ["r√©gime alimentaire √©quilibr√© pour sportif", 2, 28],
                ["am√©lioration des performances en musculation", 2, 25],
                ["prise de poids masse maigre", 2, 22]
            ],
            "max_suroptimisation": 5,
            "questions": "Quel est le mieux entre la cr√©atine et la whey ?;Est-ce qu'on peut m√©langer la cr√©atine et la whey ?;Quelle est la diff√©rence entre la cr√©atine et la prot√©ine ?;Est-ce que la cr√©atine fait prendre du muscle ?;Est-ce qu'on peut m√©langer cr√©atine et whey ?;Quand prendre son shaker de whey et de cr√©atine ?;Comment prendre de la cr√©atine et de la whey ?;Comment prendre prot√©ine et cr√©atine ?;Quand prendre la prot√©ine et la cr√©atine ?;Quel est le mieux entre cr√©atine et prot√©ine ?;Est-ce que la cr√©atine est une prot√©ine ?;Est-ce que on peut m√©langer la cr√©atine avec la prot√©ine ?;Est-ce bon de prendre de la cr√©atine ?;Est-ce que la cr√©atine augmente la masse musculaire ?;Quand voit-on les effets de la cr√©atine ?;Quels sont les effets positifs de la cr√©atine ?;Qu'est-ce que la cr√©atine ? ;La prise de cr√©atine est-elle efficace ? ;Faut-il prendre la whey et la cr√©atine en m√™me temps ? ;Faut-il n√©cessairement consommer un m√©lange de whey et de cr√©atine ?",
            "paa": [
                "Quel est le mieux entre la cr√©atine et la whey ?",
                "Est-ce qu'on peut m√©langer la cr√©atine et la whey ?", 
                "Quelle est la diff√©rence entre la cr√©atine et la prot√©ine ?",
                "Comment prendre de la cr√©atine et de la whey ?",
                "Quand prendre la prot√©ine et la cr√©atine ?",
                "Est-ce que la cr√©atine fait prendre du muscle ?"
            ],
            "related_searches": [],
            "inline_videos": [],
            "bigrams": [
                ["cr√©atine monohydrate", 8, 31],
                ["prise masse", 6, 27],
                ["whey protein", 5, 25],
                ["masse musculaire", 7, 24],
                ["compl√©ment alimentaire", 4, 23],
                ["r√©cup√©ration musculaire", 3, 21],
                ["force musculaire", 4, 18],
                ["prot√©ine whey", 4, 16],
                ["acide amin√©", 3, 15],
                ["d√©veloppement musculaire", 2, 14],
                ["nutrition sportive", 3, 13],
                ["synth√®se prot√©ines", 2, 12],
                ["performance sportive", 2, 11],
                ["musculation intensive", 2, 10],
                ["shaker prot√©ine", 2, 9],
                ["entra√Ænement intensif", 3, 8],
                ["croissance musculaire", 2, 8],
                ["endurance physique", 2, 7]
            ],
            "trigrams": [
                ["cr√©atine prise masse", 4, 32],
                ["whey cr√©atine bcaa", 3, 28],
                ["compl√©ment alimentaire musculation", 3, 25],
                ["masse musculaire rapidement", 2, 22],
                ["r√©cup√©ration musculaire optimale", 2, 20],
                ["prot√©ine whey isolate", 3, 18],
                ["cr√©atine monohydrate pure", 2, 17],
                ["force puissance musculaire", 2, 15],
                ["d√©veloppement masse maigre", 2, 14],
                ["nutrition sportive avanc√©e", 2, 13]
            ],
            "type_editorial": 100,
            "type_catalogue": 0,
            "type_fiche_produit": 0,
            "mots_uniques_min_max_moyenne": [37, 57, 49],
            "concurrence": [
                {
                    "h1": "Cr√©atine vs Whey : Le Guide Complet 2024",
                    "title": "Cr√©atine ou Whey : Que Choisir pour Maximiser ses Gains ?",
                    "h2": "Comparaison scientifique cr√©atine et whey",
                    "h3": "Effets sur la masse musculaire",
                    "score": 72,
                    "suroptimisation": 3,
                    "position": 1,
                    "words": 1250,
                    "url": "https://www.nutrimuscle.com/creatine-vs-whey-guide",
                    "domaine": "nutrimuscle.com",
                    "internal_links": 8,
                    "external_links": 2,
                    "tableaux": 1,
                    "titles": 6,
                    "video": 1,
                    "liste": 3,
                    "image": 12,
                    "overopt_details": {"total_density": 4.2, "stuffing_count": 0, "clustering_penalty": 0, "flagged_keywords": []},
                    "keyword_density_total": 4.2,
                    "stuffing_patterns": 0,
                    "clustering_issues": 0,
                    "overopt_level": "üìä Faible",
                    "recommendations": ["Optimisation √©quilibr√©e - aucun probl√®me majeur d√©tect√©"]
                },
                {
                    "h1": "Quels sont les bienfaits de la cr√©atine sur la prise de masse ?",
                    "title": "Cr√©atine et prise de masse : √† quoi s'attendre r√©ellement ?",
                    "h2": "Comment prendre de la cr√©atine pour faire une prise de masse ?",
                    "h3": "Quand prendre de la cr√©atine pour une prise de masse ?",
                    "score": 64,
                    "suroptimisation": 8,
                    "position": 2,
                    "words": 1075,
                    "url": "https://nutriandco.com/fr/pages/creatine-prise-de-masse",
                    "domaine": "nutriandco.com",
                    "internal_links": 12,
                    "external_links": 0,
                    "tableaux": 0,
                    "titles": 8,
                    "video": 0,
                    "liste": 2,
                    "image": 54,
                    "overopt_details": {"total_density": 6.8, "stuffing_count": 0, "clustering_penalty": 2, "flagged_keywords": []},
                    "keyword_density_total": 6.8,
                    "stuffing_patterns": 0,
                    "clustering_issues": 2,
                    "overopt_level": "üìä Faible",
                    "recommendations": ["Optimisation √©quilibr√©e - aucun probl√®me majeur d√©tect√©"]
                },
                {
                    "h1": "Whey ou Cr√©atine : Quel Compl√©ment Choisir ?",
                    "title": "Whey vs Cr√©atine : Diff√©rences et Conseils d'Utilisation",
                    "h2": "Les avantages de la whey prot√©ine",
                    "h3": "Les b√©n√©fices de la cr√©atine monohydrate",
                    "score": 58,
                    "suroptimisation": 12,
                    "position": 3,
                    "words": 890,
                    "url": "https://www.myprotein.fr/blog/supplements/whey-vs-creatine",
                    "domaine": "myprotein.fr",
                    "internal_links": 15,
                    "external_links": 1,
                    "tableaux": 2,
                    "titles": 7,
                    "video": 0,
                    "liste": 4,
                    "image": 8,
                    "overopt_details": {"total_density": 9.1, "stuffing_count": 1, "clustering_penalty": 3, "flagged_keywords": []},
                    "keyword_density_total": 9.1,
                    "stuffing_patterns": 1,
                    "clustering_issues": 3,
                    "overopt_level": "üìä Faible",
                    "recommendations": ["Distribuer les mots-cl√©s plus naturellement"]
                },
                {
                    "h1": "Guide Complet : Cr√©atine et Whey pour la Musculation",
                    "title": "Cr√©atine + Whey : La Combinaison Parfaite ?",
                    "h2": "Peut-on m√©langer cr√©atine et whey ?",
                    "h3": "Dosage optimal cr√©atine et prot√©ines",
                    "score": 45,
                    "suroptimisation": 15,
                    "position": 4,
                    "words": 1420,
                    "url": "https://www.bodybuilding.fr/creatine-whey-guide",
                    "domaine": "bodybuilding.fr",
                    "internal_links": 6,
                    "external_links": 3,
                    "tableaux": 1,
                    "titles": 9,
                    "video": 2,
                    "liste": 1,
                    "image": 18,
                    "overopt_details": {"total_density": 11.4, "stuffing_count": 2, "clustering_penalty": 5, "flagged_keywords": [{"keyword": "cr√©atine", "frequency": 8, "density": 2.8, "issues": ["Densit√© √©lev√©e (>2.2%)"]}]},
                    "keyword_density_total": 11.4,
                    "stuffing_patterns": 2,
                    "clustering_issues": 5,
                    "overopt_level": "üìä Faible",
                    "recommendations": ["Distribuer les mots-cl√©s plus naturellement", "√âliminer les patterns de keyword stuffing"]
                },
                {
                    "h1": "Cr√©atine ou Prot√©ine : Que Prendre en Premier ?",
                    "title": "Cr√©atine vs Prot√©ine : Guide du D√©butant 2024",
                    "h2": "Diff√©rences entre cr√©atine et whey protein",
                    "h3": "Quel budget pour ces compl√©ments ?",
                    "score": 38,
                    "suroptimisation": 22,
                    "position": 5,
                    "words": 756,
                    "url": "https://musculation.ooreka.fr/astuce/voir/735543/creatine-ou-proteine",
                    "domaine": "musculation.ooreka.fr",
                    "internal_links": 3,
                    "external_links": 0,
                    "tableaux": 0,
                    "titles": 4,
                    "video": 0,
                    "liste": 1,
                    "image": 6,
                    "overopt_details": {"total_density": 15.2, "stuffing_count": 3, "clustering_penalty": 8, "flagged_keywords": [{"keyword": "cr√©atine", "frequency": 12, "density": 3.8, "issues": ["Densit√© critique (>3.5%)", "Fr√©quence excessive (>12)"]}, {"keyword": "prot√©ine", "frequency": 9, "density": 2.4, "issues": ["Densit√© √©lev√©e (>2.2%)"]}]},
                    "keyword_density_total": 15.2,
                    "stuffing_patterns": 3,
                    "clustering_issues": 8,
                    "overopt_level": "‚ö° Mod√©r√©",
                    "recommendations": ["R√©duire la densit√© de 'cr√©atine' (<2%)", "R√©duire la densit√© de 'prot√©ine' (<2%)", "Distribuer les mots-cl√©s plus naturellement", "√âliminer les patterns de keyword stuffing"]
                }
            ]
        } 