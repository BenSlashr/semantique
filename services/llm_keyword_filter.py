"""
Service de filtrage des mots-clÃ©s parasites via LLM GPT-5-Nano
CoÃ»t: $1 pour 4000 vÃ©rifications
"""
import asyncio
import logging
import time
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
import json
import os

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    
from config import settings

# Configuration du logging
logger = logging.getLogger(__name__)

# Pour voir tous les logs de debug LLM, dÃ©commentez la ligne suivante :
# logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class LLMKeywordFilter:
    """Service de filtrage intelligent des mots-clÃ©s via GPT-5-Nano"""
    
    def __init__(self):
        self.client = None
        self.enabled = False
        self.daily_requests = 0
        self.last_reset = datetime.now().date()
        
        # Initialisation seulement si la clÃ© API et openai sont disponibles
        if (OPENAI_AVAILABLE and 
            settings.LLM_FILTERING_ENABLED and 
            settings.OPENAI_API_KEY):
            try:
                self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
                self.enabled = True
                logger.info("âœ… LLM Keyword Filter activÃ© avec GPT-5-Nano")
            except Exception as e:
                logger.warning(f"âŒ Erreur initialisation OpenAI: {e}")
                self.enabled = False
        else:
            logger.info("â„¹ï¸ LLM Keyword Filter dÃ©sactivÃ© (clÃ© API manquante ou openai non installÃ©)")
    
    async def filter_keywords_batch(self, keywords: List[str], query: str) -> List[str]:
        """
        Filtre un batch de mots-clÃ©s via GPT-5-Nano
        
        Args:
            keywords: Liste des mots-clÃ©s Ã  filtrer
            query: RequÃªte SEO originale pour le contexte
            
        Returns:
            Liste des mots-clÃ©s filtrÃ©s (sans parasites)
        """
        logger.info(f"ğŸš€ DÃ‰BUT FILTRAGE LLM: {len(keywords)} mots-clÃ©s pour la requÃªte '{query}'")
        
        if not self.enabled:
            logger.debug("âš ï¸ Service LLM dÃ©sactivÃ©, retour des mots-clÃ©s originaux")
            return keywords
            
        # VÃ©rification des limites quotidiennes
        if not self._can_make_request():
            logger.warning("ğŸš¨ Limite quotidienne LLM atteinte, fallback vers filtrage local")
            return keywords
            
        try:
            # DÃ©coupage en batches si nÃ©cessaire
            if len(keywords) > settings.LLM_BATCH_SIZE:
                filtered_results = []
                for i in range(0, len(keywords), settings.LLM_BATCH_SIZE):
                    batch = keywords[i:i + settings.LLM_BATCH_SIZE]
                    batch_result = await self._process_batch(batch, query)
                    filtered_results.extend(batch_result)
                return filtered_results
            else:
                return await self._process_batch(keywords, query)
                
        except Exception as e:
            logger.error(f"âŒ Erreur LLM filtering: {e}")
            return keywords  # Fallback vers liste originale
    
    async def _process_batch(self, keywords: List[str], query: str) -> List[str]:
        """Traite un batch de mots-clÃ©s avec GPT-5-Nano"""
        
        start_time = time.time()
        
        # Construction du prompt optimisÃ©
        prompt = self._build_prompt(keywords, query)
        
        try:
            # Logging de la requÃªte envoyÃ©e
            logger.debug(f"ğŸ“¤ PROMPT ENVOYÃ‰ Ã€ GPT-5-NANO:\n{prompt}")
            logger.debug(f"ğŸ“Š ParamÃ¨tres: model={settings.LLM_MODEL}, effort=low, verbosity=low")
            
            # Appel Ã  l'API GPT-5-Nano
            response = self.client.responses.create(
                model=settings.LLM_MODEL,
                input=prompt,
                reasoning={"effort": "low"},  # CoÃ»t rÃ©duit
                text={"verbosity": "low"}     # RÃ©ponse concise
            )
            
            # Logging de la rÃ©ponse brute
            logger.debug(f"ğŸ“¥ RÃ‰PONSE BRUTE GPT-5-NANO:\n{response.output_text}")
            
            # IncrÃ©ment du compteur de requÃªtes
            self._increment_request_counter()
            
            # Parse de la rÃ©ponse
            filtered_keywords = self._parse_response(response.output_text)
            
            # Logging des mÃ©triques et rÃ©sultat final
            processing_time = time.time() - start_time
            logger.info(f"ğŸ¤– LLM filtrage: {len(keywords)} â†’ {len(filtered_keywords)} mots-clÃ©s ({processing_time:.2f}s)")
            logger.debug(f"ğŸ“‹ MOTS-CLÃ‰S AVANT: {keywords}")
            logger.debug(f"ğŸ“‹ MOTS-CLÃ‰S APRÃˆS: {filtered_keywords}")
            
            return filtered_keywords
            
        except Exception as e:
            logger.error(f"âŒ Erreur API GPT-5-Nano: {e}")
            logger.debug(f"ğŸ” DÃ©tails erreur: {type(e).__name__} - {str(e)}")
            logger.debug(f"ğŸ“ Prompt qui a causÃ© l'erreur:\n{prompt}")
            raise
    
    def _build_prompt(self, keywords: List[str], query: str) -> str:
        """Construit le prompt optimisÃ© pour le filtrage"""
        
        keywords_str = ", ".join(keywords)
        
        prompt = f"""FILTRAGE SEO - RequÃªte: "{query}"

Mots-clÃ©s extraits: {keywords_str}

TÃ‚CHE: Filtre les mots-clÃ©s parasites pour cette requÃªte SEO.

GARDE seulement les mots-clÃ©s pertinents pour le rÃ©fÃ©rencement naturel de cette requÃªte spÃ©cifique.

SUPPRIME:
- Navigation (accueil, menu, contact)
- Dates et temporel (2024, 2025, nouveau, rÃ©cent)
- Marques et noms propres non liÃ©s
- Mots trop gÃ©nÃ©riques (bien, faire, avoir)
- URLs et techniques (www, http, html)
- GÃ©ographiques gÃ©nÃ©riques si non pertinents

RETOURNE: Uniquement la liste des mots-clÃ©s valides, sÃ©parÃ©s par des virgules, sans explication."""

        return prompt
    
    def _parse_response(self, response_text: str) -> List[str]:
        """Parse la rÃ©ponse du LLM et extrait les mots-clÃ©s filtrÃ©s"""
        
        logger.debug(f"ğŸ” PARSING RÃ‰PONSE LLM...")
        
        # Nettoyage de la rÃ©ponse
        response_text = response_text.strip()
        logger.debug(f"ğŸ“ RÃ©ponse nettoyÃ©e: '{response_text}'")
        
        # Si la rÃ©ponse contient des explications, prendre seulement la liste
        lines = response_text.split('\n')
        keywords_line = ""
        
        logger.debug(f"ğŸ“„ Nombre de lignes dans la rÃ©ponse: {len(lines)}")
        
        for i, line in enumerate(lines):
            logger.debug(f"ğŸ“„ Ligne {i}: '{line}'")
            if ',' in line and not line.startswith(('GARDE', 'SUPPRIME', 'TÃ‚CHE', 'RETOURNE')):
                keywords_line = line
                logger.debug(f"âœ… Ligne avec mots-clÃ©s dÃ©tectÃ©e: '{keywords_line}'")
                break
        
        if not keywords_line:
            keywords_line = response_text
            logger.debug(f"âš ï¸ Aucune ligne spÃ©cifique trouvÃ©e, utilisation de toute la rÃ©ponse")
        
        # Extraction des mots-clÃ©s
        raw_keywords = [kw.strip() for kw in keywords_line.split(',') if kw.strip()]
        logger.debug(f"ğŸ“‹ Mots-clÃ©s extraits bruts: {raw_keywords}")
        
        # Nettoyage final
        filtered_keywords = []
        for kw in raw_keywords:
            # Supprime les guillemets et autres caractÃ¨res parasites
            clean_kw = kw.strip('"\'()[]{}').lower()
            if clean_kw and len(clean_kw) > 2:
                filtered_keywords.append(clean_kw)
                logger.debug(f"âœ… Mot-clÃ© conservÃ©: '{clean_kw}'")
            else:
                logger.debug(f"âŒ Mot-clÃ© rejetÃ©: '{clean_kw}' (trop court ou vide)")
        
        logger.debug(f"ğŸ¯ PARSING TERMINÃ‰: {len(filtered_keywords)} mots-clÃ©s finaux")
        return filtered_keywords
    
    def _can_make_request(self) -> bool:
        """VÃ©rifie si on peut faire une requÃªte (limites quotidiennes)"""
        
        # Reset quotidien
        today = datetime.now().date()
        if today > self.last_reset:
            self.daily_requests = 0
            self.last_reset = today
        
        return self.daily_requests < settings.LLM_MAX_DAILY_REQUESTS
    
    def _increment_request_counter(self):
        """IncrÃ©mente le compteur de requÃªtes quotidiennes"""
        self.daily_requests += 1
    
    async def is_service_available(self) -> bool:
        """Health check du service LLM"""
        
        if not self.enabled:
            return False
            
        try:
            # Test simple avec un mot-clÃ©
            test_result = await self.filter_keywords_batch(["test"], "test query")
            return len(test_result) >= 0  # SuccÃ¨s si pas d'exception
        except:
            return False
    
    def get_daily_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques d'usage quotidiennes"""
        
        return {
            "enabled": self.enabled,
            "daily_requests": self.daily_requests,
            "remaining_requests": max(0, settings.LLM_MAX_DAILY_REQUESTS - self.daily_requests),
            "last_reset": self.last_reset.isoformat(),
            "estimated_daily_cost": round(self.daily_requests * 0.00025, 4)  # $1/4000 = $0.00025 par requÃªte
        }

# Instance globale (optionnelle)
print(f"ğŸ” llm_keyword_filter.py: settings.LLM_FILTERING_ENABLED = {settings.LLM_FILTERING_ENABLED}")
print(f"ğŸ” llm_keyword_filter.py: settings.OPENAI_API_KEY prÃ©sente = {'âœ…' if settings.OPENAI_API_KEY else 'âŒ'}")

llm_filter = LLMKeywordFilter() if settings.LLM_FILTERING_ENABLED else None
print(f"ğŸ” llm_keyword_filter.py: Instance crÃ©Ã©e = {llm_filter is not None}")
print(f"ğŸ” llm_keyword_filter.py: Instance enabled = {llm_filter.enabled if llm_filter else 'N/A'}")