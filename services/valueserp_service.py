import httpx
import os
from typing import Dict, List, Any
import asyncio
from bs4 import BeautifulSoup
import trafilatura
from trafilatura.settings import use_config
from trafilatura import extract_metadata
from .cache_service import cache_service

class ValueSerpService:
    def __init__(self):
        self.api_key = os.getenv("VALUESERP_API_KEY") or os.getenv("SERP_API_KEY")
        self.base_url = "https://api.valueserp.com/search"
        
    async def get_serp_data(self, query: str, location: str = "France", language: str = "fr", num_results: int = 20) -> Dict[str, Any]:
        """
        R√©cup√®re les donn√©es SERP via ValueSERP API avec cache 7 jours

        Args:
            query: Requ√™te de recherche
            location: Localisation g√©ographique (ex: "France")
            language: Code langue (ex: "fr")
            num_results: Nombre de r√©sultats √† r√©cup√©rer (10-30)

        Returns:
            Dictionnaire contenant organic_results, paa, related_searches, inline_videos
        """
        
        # üöÄ CACHE: V√©rification du cache d'abord
        cached_result = cache_service.get("serp", query, location, language, num_results)
        if cached_result is not None:
            print(f"üì¶ Cache HIT: SERP '{query}' (√©conomie API + scraping)")
            return cached_result

        params = {
            "api_key": self.api_key,
            "q": query,
            "location": location,
            "google_domain": "google.fr",
            "gl": "fr",
            "hl": language,
            "num": num_results,  # MODIFI√â : dynamique au lieu de 10
            "device": "desktop",
            "include_answer_box": "true",
            "include_people_also_ask": "true"
        }
        
        # V√©rification de la cl√© API
        if not self.api_key:
            error_msg = "‚ùå ERREUR: Cl√© API ValueSERP manquante!"
            print(error_msg)
            print("Cr√©ez un fichier .env avec: SERP_API_KEY=votre_cl√©")
            print("Ou configurez la variable d'environnement VALUESERP_API_KEY")
            raise Exception("Cl√© API ValueSERP non configur√©e. V√©rifiez votre fichier .env")
        
        print(f"üîç Recherche SERP pour: {query}")
        print(f"üîë Cl√© API configur√©e: {self.api_key[:10]}...")
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.get(self.base_url, params=params)
                print(f"üì° Statut r√©ponse: {response.status_code}")
                response.raise_for_status()
                serp_data = response.json()
                
                # Debug des donn√©es re√ßues
                print(f"üìä Donn√©es re√ßues - organic_results: {len(serp_data.get('organic_results', []))}")
                print(f"üìä Donn√©es re√ßues - people_also_ask: {len(serp_data.get('people_also_ask', []))}")
                
                # Debug des donn√©es (comment√© pour √©viter les probl√®mes de fichiers)
                # import json
                # try:
                #     with open('debug_serp_data.json', 'w', encoding='utf-8') as f:
                #         json.dump(serp_data, f, indent=2, ensure_ascii=False)
                #     print("üìÑ Donn√©es SERP export√©es dans debug_serp_data.json")
                # except:
                #     pass
                
                if not serp_data.get('organic_results'):
                    error_msg = f"‚ö†Ô∏è Aucun r√©sultat organique trouv√© pour '{query}'"
                    print(error_msg)
                    print("V√©rifiez que votre cl√© API ValueSERP est valide et active")
                    raise Exception(f"Aucun r√©sultat SERP trouv√© pour la requ√™te: {query}")

                # ‚≠ê NOUVEAU : Utiliser le scraping parall√®le
                organic_results = await self._process_serp_results_parallel(serp_data, num_results)
                
                # Extraction de tous les √©l√©ments SERP
                paa_questions = self._extract_paa(serp_data)
                related_searches = self._extract_related_searches(serp_data)
                inline_videos = self._extract_inline_videos(serp_data)
                
                final_result = {
                    'organic_results': organic_results,
                    'paa': paa_questions,
                    'related_searches': related_searches,
                    'inline_videos': inline_videos
                }
                
                # üíæ CACHE: Stocker le r√©sultat pour 7 jours
                cache_service.set("serp", final_result, query, location, language, num_results)
                print(f"üíæ Cache MISS: SERP '{query}' ‚Üí stock√© 7j")
                
                return final_result
                
            except httpx.HTTPError as e:
                error_msg = f"Erreur HTTP lors de l'appel √† ValueSERP: {e}"
                print(error_msg)
                print("V√©rifiez votre cl√© API ValueSERP dans le fichier .env")
                print("Ou v√©rifiez votre connexion internet")
                raise Exception(f"Erreur de connexion √† l'API ValueSERP: {e}")
            except Exception as e:
                error_msg = f"Erreur g√©n√©rale lors de l'appel ValueSERP: {e}"
                print(error_msg)
                if 'serp_data' in locals():
                    print(f"Donn√©es re√ßues: {serp_data}")
                raise Exception(f"Erreur lors de l'analyse SERP: {e}")
    
    async def _process_serp_results(self, serp_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Traite les r√©sultats SERP pour extraire les informations n√©cessaires (DEPRECATED - utiliser _process_serp_results_parallel)"""
        results = []

        if "organic_results" in serp_data:
            for result in serp_data["organic_results"]:
                processed_result = {
                    "position": result.get("position", 0),
                    "title": result.get("title", ""),
                    "url": result.get("link", ""),
                    "snippet": result.get("snippet", ""),
                    "domain": self._extract_domain(result.get("link", "")),
                }

                # R√©cup√©ration du contenu de la page
                content_data = await self._fetch_page_content(result.get("link", ""))
                processed_result.update(content_data)

                results.append(processed_result)

        return results

    async def _process_serp_results_parallel(self, serp_data: Dict[str, Any], max_results: int = 10) -> List[Dict[str, Any]]:
        """
        Traite les r√©sultats SERP en PARALL√àLE pour extraire les informations n√©cessaires

        Args:
            serp_data: Donn√©es brutes de ValueSERP
            max_results: Nombre maximum de r√©sultats √† traiter (10 ou 20)

        Returns:
            Liste des r√©sultats trait√©s avec contenu extrait
        """
        if "organic_results" not in serp_data:
            return []

        organic_results = serp_data["organic_results"][:max_results]

        # Cr√©ation des t√¢ches parall√®les
        tasks = []
        for result in organic_results:
            base_data = {
                "position": result.get("position", 0),
                "title": result.get("title", ""),
                "url": result.get("link", ""),
                "snippet": result.get("snippet", ""),
                "domain": self._extract_domain(result.get("link", "")),
            }

            # T√¢che de scraping pour cette page
            task = self._fetch_and_merge_content(result.get("link", ""), base_data)
            tasks.append(task)

        # Ex√©cution en PARALL√àLE
        print(f"üöÄ Lancement du scraping parall√®le de {len(tasks)} pages...")
        import time
        start_time = time.time()

        results = await asyncio.gather(*tasks, return_exceptions=True)

        elapsed = time.time() - start_time
        print(f"‚úÖ Scraping parall√®le termin√© en {elapsed:.2f}s")

        # Filtrage des erreurs
        valid_results = []
        error_count = 0

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                error_count += 1
                print(f"‚ö†Ô∏è Erreur page #{i+1}: {str(result)[:100]}")
                # Ajouter un r√©sultat vide pour maintenir la coh√©rence
                valid_results.append({
                    "position": i + 1,
                    "title": organic_results[i].get("title", ""),
                    "url": organic_results[i].get("link", ""),
                    "snippet": organic_results[i].get("snippet", ""),
                    "domain": self._extract_domain(organic_results[i].get("link", "")),
                    "content": "",
                    "word_count": 0,
                    "h1": "",
                    "h2": "",
                    "h3": "",
                    "author": "",
                    "date": "",
                    "description": "",
                    "sitename": "",
                    "language": "fr",
                    "internal_links": 0,
                    "external_links": 0,
                    "images": 0,
                    "tables": 0,
                    "lists": 0,
                    "videos": 0,
                    "titles": 0,
                    "content_quality": "error",
                    "scraping_error": True
                })
            else:
                valid_results.append(result)

        print(f"üìä R√©sultats valides: {len(valid_results) - error_count}/{len(results)}")

        return valid_results

    async def _fetch_and_merge_content(self, url: str, base_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        R√©cup√®re le contenu d'une page et fusionne avec les donn√©es de base
        Version optimis√©e avec gestion d'erreur
        """
        try:
            content_data = await self._fetch_page_content(url)
            return {**base_data, **content_data}
        except Exception as e:
            print(f"‚ùå Erreur scraping {url}: {str(e)[:50]}")
            # Retourner donn√©es de base sans contenu
            return {
                **base_data,
                "content": "",
                "word_count": 0,
                "h1": "",
                "h2": "",
                "h3": "",
                "author": "",
                "date": "",
                "description": "",
                "sitename": "",
                "language": "fr",
                "internal_links": 0,
                "external_links": 0,
                "images": 0,
                "tables": 0,
                "lists": 0,
                "videos": 0,
                "titles": 0,
                "content_quality": "error"
            }
    
    async def _fetch_page_content(self, url: str) -> Dict[str, Any]:
        """R√©cup√®re le contenu d'une page web pour analyse avec cache 7 jours"""
        
        # üöÄ CACHE: V√©rification du cache d'abord  
        cached_content = cache_service.get("content", url)
        if cached_content is not None:
            print(f"üì¶ Cache HIT: {url[:50]}...")
            return cached_content

        # OPTIMIS√â : Timeout r√©duit √† 10s (5s connexion + 10s total)
        timeout = httpx.Timeout(10.0, connect=5.0)

        # Headers am√©lior√©s pour √©viter blocage
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

        try:
            print(f"üîç R√©cup√©ration: {url[:60]}...")
            async with httpx.AsyncClient(timeout=timeout, follow_redirects=True) as client:
                response = await client.get(url, headers=headers)
                
                if response.status_code == 200:
                    html_content = response.text
                    
                    # NOUVELLE APPROCHE: Extraction avec trafilatura + m√©tadonn√©es
                    main_content = self._extract_content_with_trafilatura(html_content, url)
                    metadata = self._extract_metadata_with_trafilatura(html_content)
                    
                    # BeautifulSoup uniquement pour les statistiques HTML
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    # Comptage des mots bas√© sur le contenu extrait par trafilatura
                    word_count = len(main_content.split()) if main_content else 0
                    
                    # Validation am√©lior√©e de la qualit√©
                    content_quality = self._validate_content_quality_v2(main_content, word_count, metadata)
                    
                    result = {
                        # M√©tadonn√©es extraites par trafilatura
                        "h1": metadata.get('title') or self._extract_h1(soup),
                        "h2": self._extract_h2(soup),  # Garde l'ancienne m√©thode pour H2/H3
                        "h3": self._extract_h3(soup),
                        
                        # Contenu principal (trafilatura)
                        "content": main_content,
                        "word_count": word_count,
                        
                        # M√©tadonn√©es enrichies
                        "author": metadata.get('author', ''),
                        "date": metadata.get('date', ''),
                        "description": metadata.get('description', ''),
                        "sitename": metadata.get('sitename', ''),
                        "language": metadata.get('language', 'fr'),
                        
                        # Statistiques HTML (BeautifulSoup)
                        "internal_links": len(soup.find_all('a', href=lambda x: x and not x.startswith('http'))),
                        "external_links": len(soup.find_all('a', href=lambda x: x and x.startswith('http'))),
                        "images": len(soup.find_all('img')),
                        "tables": len(soup.find_all('table')),
                        "lists": len(soup.find_all(['ul', 'ol'])),
                        "videos": len(soup.find_all(['video', 'iframe'])),
                        "titles": len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])),
                        
                        # Qualit√© calcul√©e
                        "content_quality": content_quality
                    }
                    
                    print(f"‚úÖ OK: {word_count} mots, qualit√©: {content_quality}")
                    
                    # üíæ CACHE: Stocker le contenu si valide
                    if word_count > 0:
                        cache_service.set("content", result, url)
                        print(f"üíæ Cache MISS: {url[:50]}... ‚Üí stock√© 7j")
                    
                    return result
                else:
                    print(f"‚ö†Ô∏è HTTP {response.status_code}")
                    raise Exception(f"HTTP {response.status_code}")

        except httpx.TimeoutException:
            print(f"‚è±Ô∏è Timeout pour {url[:50]}")
            raise Exception("Timeout")
        except Exception as e:
            print(f"‚ùå Erreur: {str(e)[:50]}")
            raise
            
        return {
            "h1": "",
            "h2": "",
            "h3": "",
            "content": "",
            "word_count": 0,
            "author": "",
            "date": "",
            "description": "",
            "sitename": "",
            "language": "fr",
            "internal_links": 0,
            "external_links": 0,
            "images": 0,
            "tables": 0,
            "lists": 0,
            "videos": 0,
            "titles": 0,
            "content_quality": "failed"
        }
    
    def _validate_content_quality(self, content: str, word_count: int) -> str:
        """Valide la qualit√© du contenu extrait - Version assouplie"""
        if not content or not content.strip():
            return "empty"
        
        # Seuils beaucoup plus permissifs
        if word_count < 20:  # √âtait 50, maintenant 20
            return "too_short"
        
        if word_count < 100:  # √âtait 200, maintenant 100
            return "short"
        
        # V√©rifier si le contenu semble √™tre principalement du menu/navigation
        navigation_indicators = [
            'accueil', 'contact', 'mentions l√©gales', 'politique de confidentialit√©',
            'conditions g√©n√©rales', 'plan du site', 'newsletter', 'suivez-nous',
            'r√©seaux sociaux', 'twitter', 'facebook', 'instagram', 'linkedin'
        ]
        
        content_lower = content.lower()
        nav_matches = sum(1 for indicator in navigation_indicators if indicator in content_lower)
        
        # Seuil plus permissif pour la navigation
        if nav_matches > 8 and word_count < 200:  # √âtait 5 et 500, maintenant 8 et 200
            return "mostly_navigation"
        
        # Seuils plus permissifs pour les qualit√©s
        if word_count > 800:  # √âtait 1000
            return "excellent"
        elif word_count > 300:  # √âtait 500
            return "good"
        elif word_count > 50:  # Nouveau seuil
            return "acceptable"
        else:
            return "short"

    def _extract_domain(self, url: str) -> str:
        """Extrait le domaine d'une URL"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return ""
    
    def _extract_h1(self, soup: BeautifulSoup) -> str:
        """Extrait le H1 principal"""
        h1 = soup.find('h1')
        return h1.get_text().strip() if h1 else ""
    
    def _extract_h2(self, soup: BeautifulSoup) -> str:
        """Extrait le premier H2"""
        h2 = soup.find('h2')
        return h2.get_text().strip() if h2 else ""
    
    def _extract_h3(self, soup: BeautifulSoup) -> str:
        """Extrait le premier H3"""
        h3 = soup.find('h3')
        return h3.get_text().strip() if h3 else ""
    
    def _extract_content_with_trafilatura(self, html_content: str, url: str = "") -> str:
        """Extrait le contenu principal avec trafilatura hybride intelligent"""
        try:
            # STRAT√âGIE 1: Trafilatura mode pr√©cision (pour sites bien structur√©s)
            content_precise = self._try_trafilatura_precise(html_content, url)
            
            # STRAT√âGIE 2: Si contenu insuffisant, essayer mode agressif
            if not content_precise or len(content_precise.split()) < 100:
                print("üìÑ Contenu trafilatura pr√©cis insuffisant, essai mode agressif")
                content_aggressive = self._try_trafilatura_aggressive(html_content, url)
                
                # Prendre le plus long entre pr√©cis et agressif
                if content_aggressive and len(content_aggressive.split()) > len(content_precise.split() if content_precise else []):
                    content_precise = content_aggressive
            
            # STRAT√âGIE 3: Si toujours insuffisant, utiliser BeautifulSoup hybride
            if not content_precise or len(content_precise.split()) < 50:
                print("üìÑ Trafilatura insuffisant, utilisation BeautifulSoup hybride")
                return self._extract_content_beautifulsoup_smart(html_content)
            
            word_count = len(content_precise.split())
            print(f"üìÑ Trafilatura hybride: contenu extrait ({word_count} mots)")
            return content_precise.strip()
                
        except Exception as e:
            print(f"üìÑ Erreur trafilatura hybride: {e}, fallback BeautifulSoup")
            return self._extract_content_beautifulsoup_smart(html_content)
    
    def _try_trafilatura_precise(self, html_content: str, url: str) -> str:
        """Trafilatura mode pr√©cision"""
        try:
            config = use_config()
            config.set('DEFAULT', 'EXTRACTION_TIMEOUT', '30')
            
            content = trafilatura.extract(
                html_content,
                url=url,
                config=config,
                include_comments=False,
                include_tables=True,
                include_links=False,
                include_images=False,
                include_formatting=False,
                output_format='txt',
                target_language='fr',
                deduplicate=True,
                favor_precision=True
            )
            
            return content or ""
        except:
            return ""
    
    def _try_trafilatura_aggressive(self, html_content: str, url: str) -> str:
        """Trafilatura mode agressif (plus de contenu)"""
        try:
            config = use_config()
            config.set('DEFAULT', 'MIN_EXTRACTED_SIZE', '25')  # Seuil plus bas
            config.set('DEFAULT', 'MIN_OUTPUT_SIZE', '25')
            
            content = trafilatura.extract(
                html_content,
                url=url,
                config=config,
                include_comments=False,
                include_tables=True,
                include_links=True,        # INCLURE LIENS
                include_images=False,
                include_formatting=True,   # INCLURE FORMATAGE
                output_format='txt',
                target_language=None,      # PAS DE FILTRE LANGUE
                deduplicate=False,         # PAS DE D√âDUPLICATION
                favor_precision=False,     # PRIVIL√âGIER RAPPEL
                favor_recall=True
            )
            
            return content or ""
        except:
            return ""
    
    def _extract_content_beautifulsoup_smart(self, html_content: str) -> str:
        """BeautifulSoup intelligent avec strat√©gie progressive"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # STRAT√âGIE 1: Essai avec s√©lecteurs standards (sans suppression)
            standard_selectors = ['main', 'article', '[role="main"]', '.main-content', '.content', '.entry-content']
            
            for selector in standard_selectors:
                elements = soup.select(selector)
                if elements:
                    best_element = max(elements, key=lambda e: len(e.get_text()))
                    content = best_element.get_text(separator=' ', strip=True)
                    word_count = len(content.split())
                    
                    if word_count >= 100:  # Contenu substantiel
                        print(f"üìÑ BeautifulSoup smart: s√©lecteur standard '{selector}' ({word_count} mots)")
                        return self._smart_clean_text(content)
            
            # STRAT√âGIE 2: Body complet avec nettoyage l√©ger
            print("üìÑ BeautifulSoup smart: utilisation du body avec nettoyage l√©ger")
            
            # Copie pour nettoyage
            soup_clean = BeautifulSoup(html_content, 'html.parser')
            
            # Suppression minimale et intelligente
            elements_to_remove = [
                # Scripts et styles (obligatoire)
                'script', 'style', 'noscript',
                # Navigation tr√®s sp√©cifique seulement
                'nav.main-navigation', 'nav#main-navigation',
                '.site-navigation', '#site-navigation',
                # Footer sp√©cifique
                'footer.site-footer', '#site-footer',
                # √âl√©ments publicitaires √©vidents
                '.advertisement', '.ads', '.ad-banner'
            ]
            
            for selector in elements_to_remove:
                for element in soup_clean.select(selector):
                    element.decompose()
            
            # Extraction du body nettoy√©
            body = soup_clean.find('body')
            if body:
                content = body.get_text(separator=' ', strip=True)
                content = self._smart_clean_text(content)
                word_count = len(content.split())
                
                if word_count >= 50:
                    print(f"üìÑ BeautifulSoup smart: body nettoy√© ({word_count} mots)")
                    return content
            
            # STRAT√âGIE 3: Body brut en dernier recours
            print("üìÑ BeautifulSoup smart: body brut en dernier recours")
            raw_body = soup.find('body')
            if raw_body:
                content = raw_body.get_text(separator=' ', strip=True)
                content = self._smart_clean_text(content)
                word_count = len(content.split())
                
                print(f"üìÑ BeautifulSoup smart: body brut ({word_count} mots)")
                return content
            
            return ""
            
        except Exception as e:
            print(f"üìÑ Erreur BeautifulSoup smart: {e}")
            return ""
    
    def _smart_clean_text(self, text: str) -> str:
        """Nettoyage intelligent du texte extrait"""
        import re
        
        # Normalise les espaces
        text = re.sub(r'\s+', ' ', text)
        
        # Supprime les r√©p√©titions excessives de mots courts
        words = text.split()
        clean_words = []
        prev_word = ""
        repeat_count = 0
        
        for word in words:
            if word.lower() == prev_word.lower() and len(word) <= 4:
                repeat_count += 1
                if repeat_count < 2:  # Autorise 1 r√©p√©tition
                    clean_words.append(word)
            else:
                clean_words.append(word)
                repeat_count = 0
            prev_word = word
        
        return ' '.join(clean_words).strip()
    
    def _extract_content_fallback(self, html_content: str) -> str:
        """OBSOL√àTE: Remplac√© par _extract_content_beautifulsoup_smart"""
        # Redirection vers la nouvelle m√©thode smart
        return self._extract_content_beautifulsoup_smart(html_content)
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """Wrapper pour compatibilit√© - utilise trafilatura"""
        html_content = str(soup)
        return self._extract_content_with_trafilatura(html_content)
    
    def _count_words(self, soup: BeautifulSoup) -> int:
        """Compte le nombre de mots dans le contenu principal"""
        text = self._extract_content(soup)
        if not text.strip():
            return 0
        
        # Nettoyage du texte pour un comptage plus pr√©cis
        clean_text = ' '.join(text.split())  # Supprime les espaces multiples
        word_count = len(clean_text.split())
        
        print(f"üìä Nombre de mots dans le contenu principal: {word_count}")
        return word_count
    
    def _extract_metadata_with_trafilatura(self, html_content: str) -> Dict[str, str]:
        """Extrait les m√©tadonn√©es avec trafilatura"""
        try:
            metadata = extract_metadata(html_content)
            
            if metadata:
                return {
                    'title': metadata.title or '',
                    'author': metadata.author or '',
                    'date': str(metadata.date) if metadata.date else '',
                    'description': metadata.description or '',
                    'sitename': metadata.sitename or '',
                    'language': metadata.language or 'fr',
                    'url': metadata.url or '',
                    'categories': ', '.join(metadata.categories) if metadata.categories else '',
                    'tags': ', '.join(metadata.tags) if metadata.tags else ''
                }
            else:
                return {}
                
        except Exception as e:
            print(f"üìÑ Erreur extraction m√©tadonn√©es: {e}")
            return {}
    
    def _validate_content_quality_v2(self, content: str, word_count: int, metadata: Dict[str, str]) -> str:
        """Validation am√©lior√©e de la qualit√© du contenu avec m√©tadonn√©es"""
        if not content or not content.strip():
            return "empty"
        
        # Seuils plus r√©alistes bas√©s sur les standards web
        if word_count < 30:
            return "too_short"
        elif word_count < 100:
            return "short"
        elif word_count < 300:
            # V√©rifie si c'est un contenu structur√© (avec m√©tadonn√©es)
            if metadata.get('author') or metadata.get('date') or len(metadata.get('title', '')) > 10:
                return "acceptable"  # Contenu court mais structur√©
            else:
                return "short"
        elif word_count < 800:
            return "good"
        elif word_count < 2000:
            return "excellent"
        else:
            # Tr√®s long - v√©rifie la structure
            if metadata.get('author') and metadata.get('date'):
                return "comprehensive"  # Article long et bien structur√©
            else:
                return "excellent"
        
        # Indicateurs de qualit√© suppl√©mentaires
        quality_indicators = 0
        
        # Pr√©sence d'auteur
        if metadata.get('author'):
            quality_indicators += 1
        
        # Date de publication
        if metadata.get('date'):
            quality_indicators += 1
        
        # Description m√©ta
        if len(metadata.get('description', '')) > 50:
            quality_indicators += 1
        
        # Titre significatif
        if len(metadata.get('title', '')) > 20:
            quality_indicators += 1
        
        # Site identifi√©
        if metadata.get('sitename'):
            quality_indicators += 1
        
        # Ajustement bas√© sur les indicateurs de qualit√©
        if quality_indicators >= 3 and word_count >= 200:
            return "professional"  # Contenu professionnel bien structur√©
        
        return "good" if word_count >= 300 else "acceptable"
    
    def _count_words_from_content(self, content: str) -> int:
        """Compte les mots directement depuis le contenu extrait"""
        if not content or not content.strip():
            return 0
        
        # Nettoyage et comptage plus pr√©cis
        clean_content = ' '.join(content.split())  # Normalise les espaces
        word_count = len(clean_content.split())
        
        print(f"üìä Nombre de mots dans le contenu extrait: {word_count}")
        return word_count
    
    def _extract_paa(self, serp_data: Dict[str, Any]) -> List[str]:
        """Extrait les questions People Also Ask (related_questions dans ValueSERP)"""
        paa_questions = []
        
        print(f"üîç Cl√©s dans serp_data: {list(serp_data.keys())}")
        
        # ValueSERP utilise "related_questions" pour les PAA
        if "related_questions" in serp_data and serp_data["related_questions"]:
            print(f"‚úÖ Trouv√© related_questions avec {len(serp_data['related_questions'])} √©l√©ments")
            
            for paa_item in serp_data["related_questions"]:
                if isinstance(paa_item, dict):
                    if "question" in paa_item:
                        paa_questions.append(paa_item["question"])
                        print(f"üìã Question extraite: {paa_item['question']}")
                    elif "title" in paa_item:
                        paa_questions.append(paa_item["title"])
        else:
            print("‚ùå Aucune related_questions trouv√©e")
        
        print(f"üìã Total PAA extraites: {len(paa_questions)}")
        return paa_questions
    
    def _extract_related_searches(self, serp_data: Dict[str, Any]) -> List[str]:
        """Extrait les recherches associ√©es"""
        related_searches = []
        
        if "related_searches" in serp_data and serp_data["related_searches"]:
            print(f"‚úÖ Trouv√© related_searches avec {len(serp_data['related_searches'])} √©l√©ments")
            
            for search_item in serp_data["related_searches"]:
                if isinstance(search_item, dict):
                    if "query" in search_item:
                        related_searches.append(search_item["query"])
                    elif "title" in search_item:
                        related_searches.append(search_item["title"])
                elif isinstance(search_item, str):
                    related_searches.append(search_item)
        
        return related_searches
    
    def _clean_wikipedia_text(self, text: str) -> str:
        """Nettoie le texte sp√©cifique √† Wikipedia"""
        import re
        
        # Patterns sp√©cifiques √† Wikipedia √† supprimer
        wikipedia_patterns = [
            r'\[modifier\s*\|\s*modifier\s+le\s+code\]',  # [modifier | modifier le code]
            r'\[modifier\]',                               # [modifier]
            r'\[modifier\s+le\s+code\]',                  # [modifier le code]
            r'\[r√©f\.\s*souhait√©e\]',                     # [r√©f. souhait√©e]
            r'\[r√©f\.\s*n√©cessaire\]',                    # [r√©f. n√©cessaire]
            r'\[citation\s*n√©cessaire\]',                 # [citation n√©cessaire]
            r'\[source\s*insuffisante\]',                 # [source insuffisante]
            r'\[Quand\s*\?\]',                            # [Quand ?]
            r'\[O√π\s*\?\]',                               # [O√π ?]
            r'\[Qui\s*\?\]',                              # [Qui ?]
            r'\[Comment\s*\?\]',                          # [Comment ?]
            r'\[Pourquoi\s*\?\]',                         # [Pourquoi ?]
            r'\[style\s*√†\s*revoir\]',                    # [style √† revoir]
            r'\[pas\s*clair\]',                           # [pas clair]
            r'\[pr√©cision\s*n√©cessaire\]',                # [pr√©cision n√©cessaire]
            r'\[\d+\]',                                   # [1], [2], etc. (r√©f√©rences)
            r'Article\s*d√©taill√©\s*:',                    # Article d√©taill√© :
            r'Voir\s*aussi\s*:',                          # Voir aussi :
            r'Cat√©gories\s*:',                            # Cat√©gories :
            r'Portail\s*de\s*',                           # Portail de
            r'Ce\s*document\s*provient\s*de',             # Ce document provient de
            r'Derni√®re\s*modification\s*de\s*cette\s*page', # Derni√®re modification
            r'R√©cup√©r√©e\s*de\s*¬´\s*https?://[^¬ª]*¬ª',      # R√©cup√©r√©e de ¬´ URL ¬ª
            r'Espaces\s*de\s*noms',                       # Espaces de noms
            r'Affichages',                                # Affichages
            r'Outils',                                    # Outils
            r'Imprimer\s*/\s*exporter',                   # Imprimer / exporter
            r'Dans\s*d\'autres\s*projets',               # Dans d'autres projets
            r'Dans\s*d\'autres\s*langues',               # Dans d'autres langues
            r'Wikimedia\s*Commons',                       # Wikimedia Commons
            r'Wiktionnaire',                              # Wiktionnaire
            r'Wikiquote',                                 # Wikiquote
            r'Wikinews',                                  # Wikinews
            r'Wikiversit√©',                               # Wikiversit√©
            r'Wikibooks',                                 # Wikibooks
            r'Wikisource',                                # Wikisource
            r'Wikivoyage',                                # Wikivoyage
        ]
        
        # Applique tous les patterns
        for pattern in wikipedia_patterns:
            text = re.sub(pattern, ' ', text, flags=re.IGNORECASE)
        
        # Nettoie les espaces multiples
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _extract_inline_videos(self, serp_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extrait les vid√©os int√©gr√©es"""
        videos = []
        
        if "inline_videos" in serp_data and serp_data["inline_videos"]:
            print(f"‚úÖ Trouv√© inline_videos avec {len(serp_data['inline_videos'])} √©l√©ments")
            
            for video_item in serp_data["inline_videos"]:
                if isinstance(video_item, dict):
                    video_data = {
                        "title": video_item.get("title", ""),
                        "link": video_item.get("link", ""),
                        "thumbnail": video_item.get("thumbnail", ""),
                        "duration": video_item.get("duration", ""),
                        "source": video_item.get("source", "")
                    }
                    videos.append(video_data)
        
        return videos
    
    def _get_demo_data(self, query: str) -> Dict[str, Any]:
        """Retourne des donn√©es de d√©monstration adapt√©es √† la requ√™te"""
        
        # Donn√©es adapt√©es selon la requ√™te
        if "seo" in query.lower() and "lille" in query.lower():
            return {
                'organic_results': [
                    {
                        "position": 1,
                        "title": "Agence SEO Lille - R√©f√©rencement naturel et digital",
                        "url": "https://www.agence-seo-lille.fr/",
                        "domain": "agence-seo-lille.fr",
                        "h1": "Agence SEO sp√©cialis√©e √† Lille",
                        "h2": "Services de r√©f√©rencement naturel",
                        "h3": "Audit SEO gratuit",
                        "word_count": 1200,
                        "internal_links": 15,
                        "external_links": 3,
                        "images": 8,
                        "tables": 1,
                        "lists": 5,
                        "videos": 0,
                        "titles": 6,
                        "content": "Agence SEO Lille sp√©cialis√©e r√©f√©rencement naturel digital marketing. Notre agence SEO √† Lille propose des services de r√©f√©rencement naturel complets pour am√©liorer votre visibilit√© sur Google. Experts en SEO depuis 10 ans, nous accompagnons les entreprises lilloises dans leur strat√©gie de r√©f√©rencement naturel. Audit SEO gratuit, optimisation technique, r√©daction de contenu SEO, netlinking... Notre √©quipe SEO ma√Ætrise tous les aspects du r√©f√©rencement naturel pour positionner votre site en premi√®re page de Google. Contactez notre agence SEO √† Lille pour un devis personnalis√©."
                    },
                    {
                        "position": 2,
                        "title": "R√©f√©rencement SEO Lille - Consultant expert",
                        "url": "https://consultant-seo-lille.com/",
                        "domain": "consultant-seo-lille.com",
                        "h1": "Consultant SEO √† Lille",
                        "h2": "Expertise en r√©f√©rencement naturel",
                        "h3": "Strat√©gie SEO personnalis√©e",
                        "word_count": 980,
                        "internal_links": 12,
                        "external_links": 5,
                        "images": 6,
                        "tables": 0,
                        "lists": 3,
                        "videos": 1,
                        "titles": 4,
                        "content": "Consultant SEO Lille expert r√©f√©rencement naturel Google. Sp√©cialiste SEO ind√©pendant bas√© √† Lille, j'aide les entreprises √† am√©liorer leur r√©f√©rencement naturel sur Google. Mes prestations SEO incluent l'audit technique, l'optimisation on-page, la strat√©gie de contenu SEO et le netlinking. Fort de 8 ans d'exp√©rience en SEO, je vous accompagne dans votre strat√©gie de r√©f√©rencement naturel pour obtenir plus de trafic qualifi√©. Consultant SEO certifi√© Google, je propose des services personnalis√©s adapt√©s √† vos objectifs business. Formation SEO, accompagnement mensuel, audit SEO complet... Contactez-moi pour booster votre r√©f√©rencement naturel √† Lille."
                    }
                ],
                'paa': [
                    "Quelle est la meilleure agence SEO √† Lille ?",
                    "Combien co√ªte le r√©f√©rencement naturel √† Lille ?",
                    "Comment choisir son consultant SEO √† Lille ?",
                    "Quel est le prix d'un audit SEO √† Lille ?"
                ],
                'related_searches': [
                    "agence seo lille prix",
                    "consultant seo lille",
                    "referencement naturel lille",
                    "agence digitale lille",
                    "audit seo lille gratuit",
                    "formation seo lille",
                    "freelance seo lille",
                    "agence web lille"
                ],
                'inline_videos': [
                    {
                        "title": "Comment choisir son agence SEO √† Lille",
                        "link": "https://youtube.com/watch?v=example1",
                        "thumbnail": "https://img.youtube.com/vi/example1/hqdefault.jpg",
                        "duration": "5:32",
                        "source": "YouTube"
                    },
                    {
                        "title": "Audit SEO gratuit Lille - Tutoriel",
                        "link": "https://youtube.com/watch?v=example2",
                        "thumbnail": "https://img.youtube.com/vi/example2/hqdefault.jpg",
                        "duration": "8:15",
                        "source": "YouTube"
                    }
                ]
            }
        
        # Donn√©es par d√©faut (cr√©atine)
        return {
            'organic_results': [
            {
                "position": 2,
                "title": "Cr√©atine et prise de masse : √† quoi s'attendre r√©ellement ?",
                "url": "https://nutriandco.com/fr/pages/creatine-prise-de-masse",
                "domain": "nutriandco.com",
                "h1": "Quels sont les bienfaits de la cr√©atine sur la prise de masse ?",
                "h2": "Comment prendre de la cr√©atine pour faire une prise de masse ?",
                "h3": "Quand prendre de la cr√©atine pour une prise de masse ?",
                "word_count": 1675,
                "internal_links": 12,
                "external_links": 0,
                "images": 54,
                "tables": 0,
                "lists": 2,
                "videos": 0,
                "titles": 8,
                "content": "Contenu de d√©monstration pour l'analyse s√©mantique..."
            },
            {
                "position": 3,
                "title": "cr√©atine whey ou bcaa : quel compl√©ment choisir et pourquoi ?",
                "url": "https://www.inshape-nutrition.com/blogs/article/creatine-whey-ou-bcaa-quel-complement-choisir-et-pourquoi",
                "domain": "www.inshape-nutrition.com",
                "h1": " ",
                "h2": "Peut-on m√©langer whey et bcaa dans un seul shaker ?",
                "h3": "",
                "word_count": 906,
                "internal_links": 11,
                "external_links": 6,
                "images": 6,
                "tables": 0,
                "lists": 6,
                "videos": 0,
                "titles": 6,
                "content": "Contenu de d√©monstration pour l'analyse s√©mantique..."
            }
            ],
            'paa': [
                "Quel est le mieux entre la cr√©atine et la whey ?",
                "Est-ce qu'on peut m√©langer la cr√©atine et la whey ?",
                "Quelle est la diff√©rence entre la cr√©atine et la prot√©ine ?",
                "Comment prendre de la cr√©atine et de la whey ?"
            ],
            'related_searches': [
                "creatine monohydrate",
                "creatine whey protein",
                "creatine prise de masse",
                "creatine effets secondaires",
                "meilleure creatine",
                "creatine avant ou apres entrainement",
                "creatine dosage",
                "creatine bcaa"
            ],
            'inline_videos': [
                {
                    "title": "Cr√©atine : Comment la prendre correctement",
                    "link": "https://youtube.com/watch?v=creatine1",
                    "thumbnail": "https://img.youtube.com/vi/creatine1/hqdefault.jpg",
                    "duration": "12:45",
                    "source": "YouTube"
                },
                {
                    "title": "Cr√©atine vs Whey : Quelle diff√©rence ?",
                    "link": "https://youtube.com/watch?v=creatine2",
                    "thumbnail": "https://img.youtube.com/vi/creatine2/hqdefault.jpg",
                    "duration": "9:20",
                    "source": "YouTube"
                }
            ]
        } 